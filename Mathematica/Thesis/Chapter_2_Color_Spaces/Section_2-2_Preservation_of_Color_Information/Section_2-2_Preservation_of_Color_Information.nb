Preservation of Color Information  
The goal of the algorithm is to preserve all the information captured by the camera which relates to skin whilst discarding as much of the irrelevant information as possible. Given that edges and features often present as shadows and highlights, all the information captured in terms of luminosity will be regarded as relevant information, at least as far as the manipulation of individual pixel values is concerned. Considering the chromatic information, the importance of the pixel value will be directly determined by a Gaussian distribution. 
Knowing the range of values produced by the rotation allows us to scale the transformation to fit into the range of the destination data type. If we have RGB pixel values in a given machine data type, the amount of information contained in each of those channels is equal to the number of values accessible in that data type. For example: for 8-bit, unsigned integers, there are 256 possible values. After a rotation, we are interested in the amount of information which lies along the new axes. This is found simply by multiplying the range of the source data type by the length of the new axes found for the unit cube. To preserve all the information captured, we would therefore have to use a larger data type to store the new values. We are, however, only interested in a small region in the chromatic space. The question is, then, how to preserve the relevant information in a way consistent with the significance indicated by the aforementioned Gaussian distribution. 
None of the rotated axes have lengths less than 1 for the unit RGB cube. For this reason we've written redistribution functions which perform any necessary type conversion whilst preserving the information in a controlled way; we can keep the information where it's needed and discard it where it's irrelevant. So although this is strictly beyond the normal meaning of a color space conversion, it is addressing a connected issue and belongs in the conversion. In terms of optimization, it is also the most efficient place in the code in which to perform this adjustment, allowing us to — for the sake of example — discard the details of the colors of a duck's feathers whilst keeping the hues and tones of human skin. 
We can use a function to redistribute the information contained on the longer axis onto the shorter axis, which can be expressed in the discrete representation of that axis necessitated by internal integer data types. There are three ways in which to implement the redistribution functions:  
 Partition 
The most straightforward redistribution method is to simply preserve the information in a 1-to-1 fashion within a region. The region can be defined in terms of the distribution Gaussian by specifying a significance level in terms of the variance or the standard deviation.  
Linear 
A slightly more sophisticated method is to use a linear redistribution. A linear distribution is equivalent to partitioning given a unit gradient. However, a linear redistribution function allows for the possibility of data compression. So then, in the case where the region of interest contains more information than can be expressed in the destination data type, a linear distribution function allows an even compression of the information from source to destination.  Lee2002  
ERF (Gaussian Error Function) 
The integral of the cumulative Gaussian (i.e. Error Function) allows the redistribution of the information on the axis in a way which selectively preserves the information about a point on the axis (i.e. the mean of the Gaussian), and then progressively discard the information as it falls into the tails of the Gaussian. So, it provides a non-linear distribution of the information. The Gaussian can be seen as describing our interest in the information contained along the axis, so it's logical to use the error function to redistribute the information. The disadvantage of this is simply the computational effort involved in generating the error function.  
The error function distribution is mathematically correct, being directly related to the Gaussian fit. Computationally, there are two considerations: the numerical representation, and performance. The discrete representation of the numerics means that — for a significant number of possible distributions — distributing using the error function has little to no advantage (or indeed difference) from using a linear distribution. 
Considering the preservation of information captured, mappings with a gradient greater than 1 are undesirable because they preserve all the information whilst being informatically wasteful in that there are functionally inaccessible discrete values in the destination range. Our stated aim is to preserve the information in the image pertaining to human skin; unevenly distributing this information across a discrete data type is not only wasteful in terms of memory, but also of processing resources because subsequent processing routines will treat the data as if it has a higher fidelity than it actually does.  
To construct a distribution function, we first need to describe the relationship of the error function to the Gaussian fit, and then produce a function with the appropriate range and domain. For a Gaussian fit with an amplitude A, a mean of μ, and a standard deviation of σ, where μ and x lie in a source range  from  to . The cumulative distribution is found by integrating from  to the point x, as can be seen in (1 ):  

∫^xA e^(-((t-μ)^2/(2σ^2)))/(Sqrt[2π]σ) dt==1/2 A(erf((x-μ)/(Sqrt[2]σ))-erf(-μ/(Sqrt[2]σ)))
The Gaussian distribution and the cumulative distribution are shown in Figure XXX  for some values chosen for illustrative purposes. All that is required now is to fix the range  to  for the domain  to . 
Chapter2 Figs errorFunction.eps  
First, we determine the maximum value taken in the domain. This is simply found by evaluating the function at . It should be noted that — if the Gaussian distribution is well contained in the source domain — the maximum value should be equal to the amplitude. For the sake of simplicity, we'll ignore the amplitude of the fitted Gaussian found previously as it is not relevant to the design of the redistribution function. So, to fix the range of the distribution function, we first scale to the range 0:1 by simply dividing through by the maximum value, and then re-scale to the destination range ==- and shift by . 

dis(x)==(()(erf((x-μ)/(Sqrt[2]σ))-erf(-μ/(Sqrt[2]σ))))/(erf(-μ/(Sqrt[2]σ))-erf(-μ/(Sqrt[2]σ)))+
subsection Efficiently Implementing the Distribution Function 
Mathematically, the ERF distribution function (1 ) achieves all the stated objectives. However, on a device we are dealing with discrete numerics and limited processing power, so further analysis is required. Where we're using a discrete domain and range, the distribution is usefully divided into three characteristic behaviours: where it is constant, where it preserves all the information, and where it selectively preserves information. Looking at the distribution, this divides the source domain into five regions: two where it is effectively constant, two where it is selective, and one region around the mean where it preserves all the information. In order to design an efficient algorithm, it is useful to identify the boundaries of these five regions. 
Chapter2/Figs/EffectivlyConstantRange 
captionThe region which discards all information beyond Subscript[Λ, 2] is shown above. The shaded grid squares show the value actually taken by the discrete distribution. 
First, we need to identify where the distribution is effectively constant. This can be found by solving the following equation in the region and domain 0<=x<=1 and then generalized to the specific discrete numerics:  

(erf(μ/(Sqrt[2]σ))+erf((x-μ)/(Sqrt[2]σ)))/(erf(μ/(Sqrt[2]σ))-erf((μ-1)/(Sqrt[2]σ)))==dL  where  dL=={
FractionBox["1"],1-
FractionBox["1"]}
The solution is found for the source domain in the range 0:1 as:  

x==Sqrt[2]σ erf^-1 ((dL-1)erf(μ/(Sqrt[2]σ))-dL erf((μ-1)/(Sqrt[2]σ)))+μ
The boundaries of the regions x<Subscript[Λ, 1] and x>Subscript[Λ, 2] for x∈{⋯} or x<Subscript[λ, 1] and x>Subscript[λ, 2] for x∈{0⋯ 1} can be written using the following helpful constants of the distribution.  
 Σ^- 	 ==erf((μ-1)/(Sqrt[2]σ)) 	 Σ^+ 	 ==erf(μ/(Sqrt[2]σ)) 	 dL 	 ==
FractionBox["1"] 	 κ 	 ==
FractionBox[] 



 Subscript[λ, 1] 	 ==σ Sqrt[2] (erf^-1) ((dL-1)Σ^+-dL Σ^-)+μ 	 Subscript[λ, 2] 	 ==σ Sqrt[2] (erf^-1) ((dL-1)Σ^--dL Σ^+)+μ 
 Subscript[Λ, 1] 	 ==+(Subscript[λ, 1](μ,σ)) 	 Subscript[Λ, 2] 	 ==+(Subscript[λ, 2](μ,σ)) 


subsubsectionThe Region Which Keeps All Information To find the region where all the information in the source domain is preserved, we differentiate the distribution and solve for where the gradient is equal to the destination range over the source range. This corresponds to the point at which a unit change in the source produces a unit change in the destination range:  

(Sqrt[2/π] e^(-((x-μ)^2/(2σ^2))))/σ(erf(μ/(Sqrt[2]σ))-erf((μ-1)/(Sqrt[2]σ)))==
FractionBox[]
Rearranging for x, we find: 

x==μ±σ Sqrt[-2log(σ(erf(μ/(Sqrt[2]σ))-erf((μ-1)/(Sqrt[2]σ))))+2log(
FractionBox[])+log(2/π)]
The boundaries of the region Subscript[Ω, 1]<x<Subscript[Ω, 2] for x∈{⋯} and Subscript[ω, 1]<x<Subscript[ω, 2] for x∈{0⋯ 1} can be written using the following helpful constants of the distribution.  
 Σ^- 	 ==erf((μ-1)/(Sqrt[2]σ)) 	 Σ^+ 	 ==erf(μ/(Sqrt[2]σ)) 	 κ 	 ==
FractionBox[] 



w(μ,σ)==σ Sqrt[log(2/π)-2log(κσ(Σ^+-Σ^-))]
The equations are found in the unit source domain 0:1. It is a simple matter to scale and shift these values to give the points in a more general source domain. 

 Subscript[ω, 1](μ,σ) 	 ==μ-w(μ,σ) 	 Subscript[ω, 2](μ,σ) 	 ==μ+w(μ,σ) 
 Subscript[Ω, 1](μ,σ) 	 ==+(μ-w(μ,σ)) 	 Subscript[Ω, 2](μ,σ) 	 ==+(μ+w(μ,σ)) 


Chapter2/Figs/ExtensionToLinearRegion 
captionThe region which preserves all information extends beyond the analytic region due to the rounding involved in the discretization. The shaded grid squares show the value actually taken by the discrete distribution. The cord is the tangent to the distribution curve shifted to the next discrete unit below the curve at Subscript[Ω, 2]. The point of intersection is the extended boundary at which the distribution begins to discard information. 
One refinement can be made to these values by recognizing that the discrete distribution extends the effectively linear region past the analytic solution by rounding the values. This can be seen in XXX , where the shaded squares are the rounded values. The extended region boundary Ω p was found by numerically solving 

⌊dis(Subscript[Ω, 2])⌋+(x-Subscript[Ω, 2])==dis(x)
In the C++ code, whilst numerical routines were used MatLab and Mathematica to perform the analysis, the extended region boundary point was found by 'walking' along the distribution from the analytic point Subscript[Ω, 2] until divergence from linear behaviour became apparent. This is regarded as a simpler solution, not requiring the use of numerical library routines, and proved to be a quick and elegant solution for the C++ implementation. The extended boundaries Ω Subscript[p, 1] and Ω Subscript[p, 2] are then defined by 

 ⌈dis(Subscript[Ω, 1])⌉-Subscript[Ω, 1] 	 ==dis(Ω Subscript[p, 1])-Ω Subscript[p, 1] 	 ⌊dis(Subscript[Ω, 2])⌋-Subscript[Ω, 2] 	 ==dis(Ω Subscript[p, 2])-Ω Subscript[p, 2] 


subsubsectionThe compression ratio 
There is one final value of interest to the development of the algorithm, which is the gradient at the mean. The reason this is of interest is because we're trying to compress the relevant data as much as possible. If the destination region is small (i.e. the destination machine type is smaller than the source type), then the gradient at the mean allows us to assess the fidelity required of the source type. If it weren't for the fact that the source is the result of a rotation transformation, then there would be little purpose in assessing this value. However, it is entirely possible that the lengthening of the axes resulting from the rotation is insignificant for the desired destination type; there's no point preserving information during the rotation which is then discarded by the redistribution. The gradient is given by 

 Δ(μ,σ) 	 ==κδ(μ,σ) 	 δ(μ,σ) 	 ==(Sqrt[2]/(σ Sqrt[π](Σ^+-Σ^-))) 


The compression ratio is at most one-to-one, therefore κ<==1 and the gradient in the unit space must always be greater than one 1<=δ(μ,σ). so κ<=Δ(μ,σ)<=δ(μ,σ). 
The required fidelity in the source domain can be found using Δ in the sense that the correspondence between one information step in the source must produce a step of Δ in the destination type. For the algorithm evaluating the maximum gradient Δ allows us to be sure that Ω — the region on the x axis where all the information is to be preserved — exists if Δ>1 or tells us that the x axis can be shortened if Δ<1. In the algorithm Δ is used to find an appropriate working data type for the rotated color space and to define the axis scaling for the rotation matrix. 
We need to consider the requested compression of information alongside the spread of information caused by the rotation, and the desired focus on the specific region of interest dictated by the statistics. Each axis in the color space is to be represented by a discrete set of numbers. The size of these sets dictates the discretization of the axis, and the ratios between them indicates the spread or compression of the information they contain. We assume that the RGB axes are each discretized to the same extent, each containing  values. After the application of the un-normalized rotational transformation, the axes contain differing numbers of values given by Subscript[, 1]==  Sqrt[3], Subscript[, 2]==  Subscript[L, 2](θ) and Subscript[, 3]==  Subscript[L, 3](θ). These axis lengths preserve all the information contained in the source color space, and so are the maximum length the axis should take. The minumum length the axis may take is where the information is lost evenly throuought the axis, and corresponds to an axis length equal to the destination axis length ==.  
We can now write an algorithm which determines the necessary scaling for the axes, and whether truncation of the extreme values is significant. As this will alter  we fix the values of κ and Δ to be those for the working range ==L(θ) which preserves all information. With this value the constants are 
 K 	 ==
FractionBox[] 	 κ(θ) 	 ==(K/L(θ)) 	 Δ(μ,σ) 	 ==(K/L(θ)) δ(μ,σ) 


The length of the axis  after rescaling should be  

 (θ,μ,σ) 	 ==min{K/L(θ) δ(μ,σ),1}L(θ) 
  	 ==min{Kδ(μ,σ),L(θ)} 


The scaling  comes from the source pixel values, the remaining terms translates simply into a rotation matrix scaling 

R[θ]==min{Kδ(μ,σ),L(θ)}⊗[θ]⊗[[θ,n]]
This satisfies the requirements placed on the gradient Δ(μ,σ)>1 because: if we substitute for  in the definition for the gradient 1  

 Δ(μ,σ) 	 ==max{K/L(θ) δ(μ,σ),1} 


And the compression ratio κ is also explicitly restricted to being at most one and now is no longer defined in terms of .  
A Piecewise Approximation to the ERF Distribution 
We now have equations which give us the four points in the source domain which mark the boundaries of the five characteristic regions. We can now use them to define a piecewise function which uses the computationally problematic error function based distribution as little as possible 

pDis(x)=={  	 x<=Subscript[Λ, 1] 
 dis(x) 	 Subscript[Λ, 1]<x<Ω Subscript[p, 1] 
 x-Ω Subscript[p, 1]+dis(Ω Subscript[p, 1]) 	 Ω Subscript[p, 1]<=x<=Ω Subscript[p, 2] 
 dis(x)+Ω Subscript[p, 2]-Ω Subscript[p, 1]-dis(Ω Subscript[p, 2])+dis(Ω Subscript[p, 1]) 	 Ω Subscript[p, 2]<x<Subscript[Λ, 2] 
 dis(Subscript[Λ, 2])+Ω Subscript[p, 2]-Ω Subscript[p, 1]-dis(Ω Subscript[p, 2])+dis(Ω Subscript[p, 1]) 	 x>=Subscript[Λ, 2] 


All three distribution techniques ( XXX  , XXX  , XXX  ) described earlier are special cases of this distribution. When the distribution has a very large variance the piecewise distribution can be simplified as a linear distribution figXXX . When the distribution has a very small variance a partitioning is more appropriate fig XXX . The most interesting distributions, however, are the ones which require the use of the piecewise distribution fig XXX . 
Chapter2/Figs/useLinear captionHere the code will decide to use a linear redistribution. 
Chapter2/Figs/usePartitioning captionWith these values the code will likely decide to use partitioning if the tolerance Subscript[τ, partitioning]is greater than Subscript[Λ, 2]-Ω Subscript[p, 2]. 
Chapter2/Figs/usePiecewiseERF captionWith these values the code will decide to use a piecewise ERF based distribution (Labeled Compact Distribution in the figure).  




















Out[191]= NotebookPut[$Failed]