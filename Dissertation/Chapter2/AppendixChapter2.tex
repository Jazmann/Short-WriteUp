
\chapter{An Integer Rotation Matrix}\label{app:AnIntegerRotationMatrix}

\section{Rotation Matrix}\label{sec:RotationMatrix}

Any rotation about an axis can be represented by a 3x3 square matrix in a 3D space. Since they are invertible, they're guaranteed to be non-singular. However, as there are many ways in which to rotate an object from one position to another, or use a combination of different rotations to get to the same point, they aren't necessarily unique. For this application, we require rotation about three different axes, which can be expressed thus:


\begin{alignat}{1}
\R_x(\theta) &= \begin{bmatrix}
1 & 0 & 0 \\
0 & \cos \theta &  -\sin \theta \\[3pt]
0 & \sin \theta  &  \cos \theta \\[3pt]
\end{bmatrix} \\[6pt]
\R_y(\theta) &= \begin{bmatrix}
\cos \theta & 0 & \sin \theta \\[3pt]
0 & 1 & 0 \\[3pt]
-\sin \theta & 0 & \cos \theta \\
\end{bmatrix} \\[6pt]
\R_z(\theta) &= \begin{bmatrix}
\cos \theta &  -\sin \theta & 0 \\[3pt]
\sin \theta & \cos \theta & 0\\[3pt]
0 & 0 & 1\\
\end{bmatrix}
\end{alignat}


Any rotation which orients the RGB color space such that one of the new axes lies along the luminosity direction is sufficient; a rotation which aligns the L axis along the luminosity direction is produced by a rotation of $\frac{\pi}4$ about the red (\textbf{R}) axis, followed by a rotation of $\arctan{\frac{1}{\sqrt{2}}}$ about the green (\textbf{G}) axis. This leaves one free rotational degree of freedom about the L axis. The resulting rotation matrix is given by:

\begin{equation}\label{eq:RotationMatrix}
\R_{xyz}(\theta) =
\begin{pmatrix}
 \frac{1}{\sqrt{3}} & \frac{1}{\sqrt{3}} & \frac{1}{\sqrt{3}} \\
 -\sqrt{\frac{2}{3}} \sine{\theta +\Pii{6} } & \sqrt{\frac{2}{3}} \cos (\theta ) & -\sqrt{\frac{2}{3}} \sine{\Pii{6}-\theta } \\
 -\sqrt{\frac{2}{3}} \cos \left(\theta +\Pii{6}\right) & -\sqrt{\frac{2}{3}} \sin (\theta ) & \sqrt{\frac{2}{3}} \cos \left(\Pii{6}-\theta \right) \\
\end{pmatrix}
\end{equation}


Where $\theta$ is the remaining rotational degree of freedom.

Using the standard rotation matrices, we get a luminosity axis which spans the range $0:\sqrt{3}$. However, the length of the two remaining axes are dependent on the value of $\theta$ used. This is a problem because, ultimately, we want the axes to fit in a range of an appropriate data type. It would be more useful to have a matrix which provided the specified rotation and scaled the axis to known lengths. In the case of the luminosity, this is straightforward; simply divide by $\sqrt{3}$. In the case of the other two axes, we need an explicit form for the lengths of the axis resulting from the rotation.

Because the absolute values of the axes in the color space have no meaning, we're only interested in the position along the axis relative to its start and end, equivalent to talking about the position in the axis relative to $0:1$, compared to about $0:255$ in unsigned, 8-bit integers. The upside is that if we're rotating the cube about its corner, we're interested in the minimum and maximum values possible along the new axis direction, which will correspond to a corner of the RGB cube. With the L axis aligned along the luminosity direction, the range of the L axis is 0 to $\sqrt{3}$. The x and y axes are symmetrical, spanning a range centered on 0. The range of their values is dependent upon the remaining degree of freedom.

We need to know, in each of the axes, how far out each point is. Because we're effectively rotating a hexagon, whatever the answer is, we know the function is going to be periodic, repeating every $\frac{\pi}{3}$ radians, so we only have to solve it in the $0:\frac{\pi}{3}$ region and then generalize. First we take the coordinates of the RGB cube and perform the rotation to find the values in the new color space.


\begin{multline}\label{eq:YabCube}
  \R_{xyz}(\theta)\cdot
\begin{pmatrix}
 0 & 1 & 1 & 0 & 0 & 0 & 1 & 1 \\
 0 & 0 & 1 & 1 & 1 & 0 & 0 & 1 \\
 0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 \\
\end{pmatrix}
 = \\
\resizebox{\textwidth}{!}{%
$\displaystyle
\begin{pmatrix}
 0 & \frac{1}{\sqrt{3}} & \frac{2}{\sqrt{3}} & \frac{1}{\sqrt{3}} & \frac{2}{\sqrt{3}} & \frac{1}{\sqrt{3}} & \frac{2}{\sqrt{3}} & \sqrt{3} \\
 0 & -\sqrt{\frac{2}{3}} \sin \left(\theta +\Pii{6}\right) & \sqrt{\frac{2}{3}} \sin \left(\Pii{6}-\theta \right) & \sqrt{\frac{2}{3}} \cos (\theta ) & \sqrt{\frac{2}{3}} \sin \left(\theta +\Pii{6}\right) & -\sqrt{\frac{2}{3}} \sin \left(\Pii{6}-\theta \right) & -\sqrt{\frac{2}{3}} \cos (\theta ) & 0 \\
 0 & -\sqrt{\frac{2}{3}} \cos \left(\theta +\Pii{6}\right) & -\sqrt{\frac{2}{3}} \cos \left(\Pii{6}-\theta \right) & -\sqrt{\frac{2}{3}} \sin (\theta ) & \sqrt{\frac{2}{3}} \cos \left(\theta +\Pii{6}\right) & \sqrt{\frac{2}{3}} \cos \left(\Pii{6}-\theta \right) & \sqrt{\frac{2}{3}} \sin (\theta ) & 0 \\
\end{pmatrix}$}
\end{multline}


The extent of the new axis is found by taking the maximum and minimum values of each row, i.e. the extreme corner positions relative to each new axis. An additional symmetry of the hexagonal projection of the RGB cube allows us to say that --- whatever functional form is taken by one of the $\theta$ dependant ranges --- the other can be found by a simple phase shift. So, recognizing that the minimum value is simply $-1$ times the maximum, we have simplified the problem to solving:

\begin{equation}\label{eq:AxisRangeMinMax}
 \max\left(\pm\frac{\sin (\theta )}{\sqrt{6}}\pm\frac{\cos (\theta )}{\sqrt{2}}, \pm\sqrt{\frac{2}{3}} \sin (\theta ) \right) \quad \text{Where} \quad 0\leq \theta \leq \frac{\pi}{3}
\end{equation}

A graphical representation of the problem can be seen in Figure~\ref{fig:YABCubeEval}.

\begin{figure}[h!] %hi-res
  \centering
    \includegraphics[width=\textwidth]{Chapter2/Figs/CornersOf_theRGBCube.jpg}
    \caption{Evaluation of the color space problem. The cube in the top-left shows the positions of the axes in the rotated space; the white and black disks on the vertical axis represent the white point and black point of the luminosity axis, and the other disks represent the ends of the chromatic axes \textbf{Ca} and \textbf{Cb}. The graphs in the top-right and bottom-left show the positions of the corners of the RGB cube relative to the chromatic axes \textbf{Ca} and \textbf{Cb}. The bottom-right graphic shows the RGB cube viewed down the luminosity axis. The graphics were generated by interactive code written in Mathematica. The value of $\theta$ was an arbitrary value from a snapshot taken from the interactive graphic.}\label{fig:YABCubeEval}
\end{figure}

In the range $0:\frac{\pi}{3}$, both sin and cos are positive, therefore the axis ranges are given by the following:

\begin{tabular}{|c|c|c|}
  \hline
    & Min & Max \\ \hline
  L & \(0\) & \(\sqrt{3}\) \\
  Ca & \(- \sqrt{\frac{2}{3}} \cos \left(\Pii{6}-(\left(\theta -\Pii{6}\right) \bmod \frac{\pi }{3})\right) \)&\( \sqrt{\frac{2}{3}} \cos \left(\Pii{6}-(\left(\theta -\Pii{6}\right) \bmod \frac{\pi }{3})\right) \)\\
 Cb & \(-\sqrt{\frac{2}{3}} \cos \left(\Pii{6}-(\theta  \bmod \frac{\pi }{3})\right) \)&\( \sqrt{\frac{2}{3}} \cos \left(\Pii{6}-(\theta  \bmod \frac{\pi }{3})\right) \)\\
  \hline
\end{tabular}

The lengths of the axis after rotation are given by:

\begin{equation}\label{eq:L}
\mathbf{L}(\theta) =
\begin{pmatrix}
\sqrt{3} \\
 \sqrt{\frac{2}{3}} \sin \left(\widetilde{\vartheta}\right) + \sqrt{2} \cos \left(\widetilde{\vartheta}\right) \\  
\sqrt{\frac{2}{3}} \sin \left(\widetilde{\theta}\right) + \sqrt{2} \cos \left(\widetilde{\theta}\right) 
\end{pmatrix}
\quad \text{where}  \quad 
\begin{array}{c}
\widetilde{\vartheta} = \left(\theta - \Pii{6}\right) \bmod \frac{\pi }{3} \\
\widetilde{\theta} = \theta  \bmod \frac{\pi }{3} 
\end{array}
\end{equation}

It is convenient to produce a transformation which will result in axes of known length. Because the rotation cannot include a translation, we desire a transformation matrix which will result in the ranges $0:1$, -$\frac{1}2:\frac{1}2$, and -$\frac{1}2:\frac{1}2$. Such a transformation is easily obtained by multiplying the rotation matrix by a diagonal matrix with the reciprocal of the maximums found above placed along the diagonal. This will scale each axis to a unit length.

The normalized 'rotation' matrix is given by:


\begin{align}\label{eq:NormRxyz3}
 \nS[\theta]  &= \left(
 \begin{array}{c}
  \frac{1}{\sqrt{3}}  \\
  \frac{1}{\sqrt{\frac{2}{3}} \sin \left(\widetilde{\vartheta}\right) + \sqrt{2} \cos \left(\widetilde{\vartheta}\right)} \\
  \frac{1}{\sqrt{\frac{2}{3}} \sin \left(\widetilde{\theta}\right) + \sqrt{2} \cos \left(\widetilde{\theta}\right)  }  \\
 \end{array}
 \right) \quad \text{where}  \quad 
 \begin{array}{c}
 \widetilde{\vartheta} = \left(\theta - \Pii{6}\right) \bmod \frac{\pi }{3} \\
 \widetilde{\theta} = \theta  \bmod \frac{\pi }{3} 
 \end{array} \\
\overline{\R}_{xyz}(\theta) &= \nS[\theta] \otimes R_{xyz}(\theta) \\
&=\left(
\begin{array}{ccc}
 \frac{1}{3} & \frac{1}{3} & \frac{1}{3} \\
 -\frac{1}{2} \frac{\sin \left(\theta +\frac{\pi }{6}\right)}{\cos\left( \widetilde{\vartheta} - \frac{\pi }{6}\right)}  & 
   \frac{1}{2} \frac{\cos (\theta )}{\cos\left( \widetilde{\vartheta}  - \frac{\pi }{6}\right)} & 
   \frac{1}{2} \frac{\sin \left(\theta -\frac{\pi }{6}\right) }{\cos\left( \widetilde{\vartheta} - \frac{\pi }{6}\right)}  \\
 -\frac{1}{2} \frac{\cos \left(\theta +\frac{\pi }{6}\right)}{\cos\left( \widetilde{\theta} - \frac{\pi }{6}\right)} & 
 -\frac{1}{2} \frac{\sin (\theta ) }{\cos\left( \widetilde{\theta}  - \frac{\pi }{6}\right)} & 
   \frac{1}{2} \frac{\cos \left(\theta -\frac{\pi }{6}\right)}{\cos \left(\widetilde{\theta}  - \frac{\pi }{6}\right)} \\
\end{array}
\right)
\end{align}

This matrix is no longer technically a rotation matrix as its inverse is no longer equal to its transpose. It now equal to:
\begin{equation}\label{eq:NormRxyz3Inverse}
 \overline{\R}_{xyz}^{-1}(\theta) =
\left(   \inS[\theta] \bigotimes R_{xyz}(\theta)
\right)^{T} 
\end{equation}

\section{Factoring the Rotation}

The rotation matrix \ref{eq:RotationMatrix} can be factored by rows to give a matrix which has one as the largest element in each row. This factorisation is used to facilitate the quantisation of the rotation matrix in Chapter 2. 

The first row is simply factored by $\frac{1}{\sqrt{3}}$ to give all ones which fits our criteria. The second and third rows each sum to zero ($\uR \cdot \mathbf{1} = [\sqrt{3},0,0]$, where $\mathbf{1}$ is a vector of ones), allowing us to state that each of the rows has one element of the opposite sign to the other two elements. The discretization for the second and third rows then only requires $\rRange = 2 \; \tRange$. It is also possible to pull out common factors from the rows, resulting in all elements taking values between -1 and 1.

\begin{equation}\label{eq:RotCommonFactors}
\uR(\theta) =
\left(
\begin{array}{c}
 \frac{1}{\sqrt{3}} \\
 \sqrt{\frac{2}{3}}  \\
 \sqrt{\frac{2}{3}} \\
\end{array}
\right) \bigotimes
\begin{pmatrix}
 1 & 1 & 1 \\
 -\sin \left(\theta +\Pii{6}\right) &  \cos (\theta ) &  \sin \left(\theta -\Pii{6}\right) \\
 -\cos \left(\theta +\Pii{6}\right) & - \sin (\theta ) & \cos \left(\theta -\Pii{6}\right) \\
\end{pmatrix}
\end{equation}

To facilitate the quantization, we next factor the second and third rows such that the largest element of each row equals exactly 1. As previously mentioned, the largest element has the opposite sign to the other two elements in a given row. So, the steps required to perform the factorization are as follows:

\begin{itemize}
\item{Find the sign of each of the elements.}
\item{Determine which of those elements is of the opposite sign.}
\item{Factor each row by that largest element.}
\end{itemize}

The sign of the elements is most easily seen by looking at the rotation as phase shifted sine functions.

\begin{equation}\label{eq:RRot}
\rR[\theta]=
\begin{pmatrix}
 1 & 1 & 1 \\
 \sine{\theta -\frac{5 \pi }{6}}  & \sine{\theta +\frac{3 \pi }{6}}  & \sine{\theta -\Pii{6}}  \\
 \sine{\theta -\frac{2 \pi }{6}}  & \sine{\theta -\frac{6 \pi }{6}}  & \sine{\theta +\frac{2 \pi }{6}}  \\
\end{pmatrix}
\end{equation}

So the signs look like

\begin{equation}\label{eq:MatrixSigns}
\begin{pmatrix}
 1 & 1 & 1 \\
\raisebox{-10mm}{\includegraphics[width = 20mm]{Chapter2/Figs/SignOfrR21.jpg}}
\scalebox{0.68}[0.9]{\(
\begin{cases}
 1 & \frac{5 \pi }{6}\leq \theta <\frac{11 \pi }{6} \\
 \um 1 & \um \Pii{6}\leq \theta <\frac{5 \pi }{6} \\
\end{cases}
\)}
 & 
 \raisebox{-10mm}{\includegraphics[width = 20mm]{Chapter2/Figs/SignOfrR22.jpg}}
 \scalebox{0.68}[0.9]{\(
\begin{cases}
 1 & \um \frac{\pi }{2}\leq \theta <\frac{\pi }{2} \\
 \um 1 & \frac{\pi }{2}\leq \theta <\frac{3 \pi }{2} \\
  \end{cases}
  \)}
 & 
 \raisebox{-10mm}{\includegraphics[width = 20mm]{Chapter2/Figs/SignOfrR23.jpg}}
 \scalebox{0.68}[0.9]{\(
\begin{cases}
 1 & \Pii{6}\leq \theta <\frac{7 \pi }{6} \\
 \um 1 & \um \frac{5 \pi}{6} \leq \theta <\Pii{6} \\
\end{cases}
\)}
 \\
 \raisebox{-10mm}{\includegraphics[width = 20mm]{Chapter2/Figs/SignOfrR31.jpg}}
 \scalebox{0.68}[0.9]{\(
\begin{cases}
 1 & \frac{\pi }{3}\leq \theta <\frac{4 \pi }{3} \\
 \um 1 & \um \frac{2 \pi }{3} \leq \theta <\frac{\pi }{3} \\
\end{cases}
\)}
 & 
\raisebox{-10mm}{\includegraphics[width = 20mm]{Chapter2/Figs/SignOfrR32.jpg}}
\scalebox{0.68}[0.9]{\(
\begin{cases}
 1 & -\pi \leq \theta <0 \\
 -1 & 0\leq \theta <\pi  \\
 \end{cases}
 \)}
 & 
  \raisebox{-10mm}{\includegraphics[width = 20mm]{Chapter2/Figs/SignOfrR33.jpg}}
  \scalebox{0.68}[0.9]{\(
\begin{cases}
 1 & \um \frac{\pi }{3}\leq \theta <\frac{2 \pi }{3} \\
 -1 & \frac{2 \pi }{3}\leq \theta <\frac{5 \pi }{3} \\
\end{cases}
\)}
 \\
\end{pmatrix}
\end{equation}

Each row is a series of three sin waves each $\frac{2\pi}{3}$ out of phase with each other. The rows are $\frac{\pi}{2}$ out of phase with each other.
The largest element changes every $\frac{\pi}{6}$ radians repeating every $\pi$ radians.
\pagebreak
\begin{table}[h]
\begin{tabular}{|c|c|c|}
\hline \rule[-2ex]{0pt}{5.5ex} 
& 
\begin{tabular}{c} 
Scale : $fS[\theta]$ \\
\raisebox{-13mm}{\includegraphics[width = 26mm]{Chapter2/Figs/fRWheel.jpg}} \\
\end{tabular}
&  Factored Rotation : $fR[\theta]$ \\ 
\hline \rule[-2ex]{0pt}{5.5ex} 
\bf{A} & \( \left(\!\!\!
\begin{array}{c}
 1 \\
 \cos (\theta ) \\
 \cosine{\Pii{6}-\theta } \\
\end{array}
\!\!\!\right) \)& \(  \left(\!\!\!
\begin{smallmatrix}
 1 & 1 & 1 \\
 \um\sec (\theta ) \sine{\theta +\Pii{6}} & 1 & \sec (\theta ) \sine{\theta-\Pii{6} } \\
 \um\cosine{\theta +\Pii{6}} \secant{\Pii{6}-\theta } & \um\secant{\Pii{6}-\theta } \sin (\theta ) & 1 \\
\end{smallmatrix}
\right) \)  \\ 
\hline \rule[-2ex]{0pt}{5.5ex} \bf{B} & \( \left(\!\!\!
\begin{array}{c}
 1 \\
 \um\sine{\theta +\Pii{6}} \\
 \cosine{\Pii{6}-\theta } \\
\end{array}
\!\!\!\right) \)  &  \(  \left(\!\!\!
\begin{smallmatrix}
 1 & 1 & 1 \\
 1 & \um\cos (\theta ) \cosecant{\theta +\Pii{6}} & \cosecant{\theta +\Pii{6}} \sine{\Pii{6}-\theta } \\
 \um\cosine{\theta +\Pii{6}} \secant{\Pii{6}-\theta } & \um\secant{\Pii{6}-\theta } \sin (\theta ) & 1 \\
\end{smallmatrix}
\!\!\!\right)  \) \\ 
\hline \rule[-2ex]{0pt}{5.5ex} \bf{C} & \(  \left(\!\!\!
\begin{array}{c}
 1 \\
 \um\sine{\theta +\Pii{6}} \\
 \um\sin (\theta ) \\
\end{array}
\!\!\!\right) \)  & \(  \left(\!\!\!
\begin{smallmatrix}
 1 & 1 & 1 \\
 1 & \um\cos (\theta ) \cosecant{\theta +\Pii{6}} & \cosecant{\theta +\Pii{6}} \sine{\Pii{6}-\theta } \\
 \cosine{\theta +\Pii{6}} \csc (\theta ) & 1 & \um\cosine{\Pii{6}-\theta } \csc (\theta ) \\
\end{smallmatrix}
\!\!\!\right) \)  \\ 
\hline \rule[-2ex]{0pt}{5.5ex} \bf{D} & \( \left(\!\!\!
\begin{array}{c}
 1 \\
 \sine{\theta-\Pii{6} } \\
 \um\sin (\theta ) \\
\end{array}
\!\!\!\right) \)  & \(  \left(\!\!\!
\begin{smallmatrix}
 1 & 1 & 1 \\
 \cosecant{\Pii{6}-\theta } \sine{\theta +\Pii{6}} & \um\cos (\theta ) \cosecant{\Pii{6}-\theta } & 1 \\
 \cosine{\theta +\Pii{6}} \csc (\theta ) & 1 & -\cosine{\Pii{6}-\theta } \csc (\theta ) \\
\end{smallmatrix}
\!\!\!\right) \)  \\ 
\hline \rule[-2ex]{0pt}{5.5ex} \bf{E} & \(  \left(\!\!\!
\begin{array}{c}
 1 \\
 \sine{\theta-\Pii{6} } \\
 \um\cosine{\theta +\Pii{6}} \\
\end{array}
\!\!\!\right)\)  & \(  \left(\!\!\!
\begin{smallmatrix}
 1 & 1 & 1 \\
 \cosecant{\Pii{6}-\theta } \sine{\theta +\Pii{6}} & \um\cos (\theta ) \cosecant{\Pii{6}-\theta } & 1 \\
 1 & \secant{\theta +\Pii{6}} \sin (\theta ) & \um\cosine{\Pii{6}-\theta } \secant{\theta +\Pii{6}} \\
\end{smallmatrix}
\!\!\!\right) \)  \\ 
\hline \rule[-2ex]{0pt}{5.5ex} \bf{F} &\( 
\left(\!\!\!
\begin{array}{c}
 1 \\
 \cos (\theta ) \\
 \um\cosine{\theta +\Pii{6}} \\
\end{array}
\!\!\!\right)\)   & \( \left(\!\!\!
\begin{smallmatrix}
 1 & 1 & 1 \\
 \um\sec (\theta ) \sine{\theta +\Pii{6}} & 1 & \sec (\theta ) \sine{\theta-\Pii{6}} \\
 1 & \secant{\theta +\Pii{6}} \sin (\theta ) & \um\cosine{\Pii{6}-\theta } \secant{\theta +\Pii{6}} \\
\end{smallmatrix}
\!\!\!\right)  \)  \\ 
\hline 
\end{tabular}
\caption{Table of factored rotation matrices.}
\label{tab:factoredRotationMatrix1}
\end{table}

By defining $\theta$ as $\theta  = \Theta + \thetaA$ --- where $\thetaA = \theta \!\!\mod \Pii{6}$, and $\Theta$ is the starting value for the region in which $\theta$ lies --- and substituting in the function $\fRe[\phi] $:

\begin{align}\label{eq:fReDef}
\fRe[\phi] &= \frac{\um 1}{2} \left(1+\sqrt{3} \tan (\phi )\right) 
\end{align}

we can now rewrite the factored rotation as shown in Table \ref{tab:factoredRotationMatrix}.

\begin{table}[h]
\begin{tabular}{|c|c|c|}
\hline \rule[-2ex]{0pt}{5.5ex}  
\begin{tabular}{c} 
$\Theta$ \\
\raisebox{-13mm}{\includegraphics[width = 26mm]{Chapter2/Figs/fRWheel.jpg}} 
\end{tabular}& 
\begin{tabular}{c} 
Scale : $fS[\theta]$ \\
$\fSs[ \theta ] \otimes \fSe[ \theta ] $ \\
\end{tabular} &  
\begin{tabular}{c} 
Factored Rotation \\ $fR[\theta]$  \\ where $\thetaA = \theta \!\!\mod \Pii{6}$
\end{tabular}\\ 
\hline \rule[-2ex]{0pt}{5.5ex} 
\bf{A} : $0$& \( \left(\!\!\!

\begin{array}{c}
 1 \\  1 \\ 1\\
\end{array} \!\!\!\right) \otimes \left(\!\!\!
\begin{array}{c}
 1 \\
 \cosine {\thetaA } \\
 \cosine{\Pii{6}-\thetaA } \\
\end{array}

\!\!\!\right) \)& \(  \left(\!\!\!
\begin{array}{ccc}
 1 & 1 & 1 \\
 \fReq{\thetaA } & 1 & \fReq{-\thetaA } \\
 \fReq{\frac{\pi }{6}-\thetaA }& \fReq{\thetaA -\frac{\pi }{6}} & 1 \\
\end{array}
\right) \)  \\ 
\hline \rule[-2ex]{0pt}{5.5ex} \bf{B} : $\frac{\pi}{6}$ & \( \left(\!\!\!

\begin{array}{c}
 1 \\  -1 \\ 1\\
\end{array} \!\!\!\right) \otimes \left(\!\!\!
\begin{array}{c}
 1 \\
 \cosine{\Pii{6}-\thetaA } \\
 \cosine {\thetaA } \\
\end{array}

\!\!\!\right) \)  &  \(  \left(\!\!\!
\begin{array}{ccc}
 1 & 1 & 1 \\
 1 & \fReq{\frac{\pi }{6}-\thetaA } & \fReq{\thetaA -\frac{\pi }{6}} \\
 \fReq{-\thetaA } & \fReq{\thetaA } & 1 \\
\end{array}
\!\!\!\right)  \) \\ 
\hline \rule[-2ex]{0pt}{5.5ex} \bf{C} : $\frac{\pi}{3}$ & \(  \left(\!\!\!

\begin{array}{c}
 1 \\  -1 \\ -1\\
\end{array} \!\!\!\right) \otimes \left(\!\!\!
\begin{array}{c}
 1 \\
 \cosine {\thetaA } \\
 \cosine{\Pii{6}-\thetaA } \\
\end{array}

\!\!\!\right) \)  & \(  \left(\!\!\!
\begin{array}{ccc}
 1 & 1 & 1 \\
 1 & \fReq{-\thetaA } & \fReq{\thetaA } \\
 \fReq{\thetaA -\frac{\pi }{6}} & 1 & \fReq{\frac{\pi }{6}-\thetaA } \\
\end{array}
\!\!\!\right) \)  \\ 
\hline \rule[-2ex]{0pt}{5.5ex} \bf{D}  : $\frac{\pi}{2}$& \( \left(\!\!\!

\begin{array}{c}
 1 \\  1 \\ -1\\
\end{array} \!\!\!\right) \otimes \left(\!\!\!
\begin{array}{c}
 1 \\
 \cosine{\Pii{6}-\thetaA } \\
 \cosine {\thetaA } \\
\end{array}

\!\!\!\right) \)  & \(  \left(\!\!\!
\begin{array}{ccc}
 1 & 1 & 1 \\
 \fReq{\frac{\pi }{6}-\thetaA } & \fReq{\thetaA -\frac{\pi }{6}} & 1 \\
 \fReq{\thetaA } & 1 & \fReq{-\thetaA } \\
\end{array}
\!\!\!\right) \)  \\ 
\hline \rule[-2ex]{0pt}{5.5ex} \bf{E}  : $\frac{2 \pi}{3}$& \(  \left(\!\!\!

\begin{array}{c}
 1 \\  1 \\ 1\\
\end{array} \!\!\!\right) \otimes \left(\!\!\!
\begin{array}{c}
 1 \\
 \cosine {\thetaA } \\
 \cosine{\Pii{6}-\thetaA } \\
\end{array}

\!\!\!\right)\)  & \(  \left(\!\!\!
\begin{array}{ccc}
 1 & 1 & 1 \\
 \fReq{-\thetaA } & \fReq{\thetaA } & 1 \\
 1 & \fReq{\frac{\pi }{6}-\thetaA } & \fReq{\thetaA -\frac{\pi }{6}} \\
\end{array}
\!\!\!\right) \)  \\ 
\hline \rule[-2ex]{0pt}{5.5ex} \bf{F}  : $\frac{5 \pi}{6}$&\( 
\left(\!\!\!

\begin{array}{c}
 1 \\  -1 \\ 1\\
\end{array} \!\!\!\right) \otimes \left(\!\!\!
\begin{array}{c}
 1 \\
 \cosine{\Pii{6}-\thetaA } \\
 \cosine {\thetaA } \\
\end{array}

\!\!\!\right)\)   & \( \left(\!\!\!
\begin{array}{ccc}
 1 & 1 & 1 \\
 \fReq{\thetaA -\frac{\pi }{6}} & 1 & \fReq{\frac{\pi }{6}-\thetaA } \\
 1 & \fReq{-\thetaA } & \fReq{\thetaA } \\
\end{array}
\!\!\!\right)  \)  \\ 
\hline 
\end{tabular}
\caption{Simplified table of factored rotation matrices.}
\label{tab:factoredRotationMatrix}
\end{table}

We can now relate the functions to each other, since they are now essentially the same function, and $\thetaA$ has the same domain; the values they take have the same range, despite being in different positions in the matrix. This allows us to write the piecewise function $\fR$ independently as a function $\fRO$, which re-orders the elements of the matrix in region $\mathbf{A}$ appropriately for the other regions.


\begin{align}\label{eq:factoredMatrixDef}
\fR[\theta] & = \fRO \left[ \fRm, \theta \right]  & 
\fRm
& = 
\left(\begin{smallmatrix}
 1 & 1 & 1 \\
 \text{fRe} (\thetaA)  & 1 & \text{fRe} (-\thetaA ) \\
 \text{fRe} \left(\frac{\pi }{6}-\thetaA \right) & \text{fRe} \left(\thetaA -\frac{\pi }{6}\right) & 1 \\
\end{smallmatrix}\right)
\end{align}

%\begin{align}\label{eq:factoredMatrixDef}
%\fR[\theta] & = \fRO \left[ \fRm, \theta \right] \\
%& = \fRO \left[
%\left(\begin{smallmatrix}
% 1 & 1 & 1 \\
% \text{fRe} (\thetaA)  & 1 & \text{fRe} (-\thetaA ) \\
% \text{fRe} \left(\frac{\pi }{6}-\thetaA \right) & \text{fRe} \left(\thetaA -\frac{\pi }{6}\right) & 1 \\
%\end{smallmatrix}\right) , \theta \right]
%\end{align}

The scaling factor $\fS ( \thetaA )$ can also be further simplified by separating the sign and combining the functional parts.

\begin{align}
\fSe(\theta) 
&=
\begin{cases}
 \left(
\begin{smallmatrix}
 1 \\
 \cos \left(\frac{\pi }{6}-\thetaA\right) \\
 \cos \left(\thetaA\right) \\
\end{smallmatrix}
\right) & 
\begin{smallmatrix}
\frac{\pi }{6}\leq (\theta  \bmod \pi )<\frac{\pi }{3}\lor \\
\frac{\pi }{2}\leq (\theta  \bmod \pi )<\frac{2 \pi }{3}\lor \\
\frac{5 \pi}{6}\leq (\theta  \bmod \pi )<\pi  \\
\end{smallmatrix} \\
 \left(
\begin{smallmatrix}
 1 \\
 \cos \left(\thetaA\right) \\
 \cos \left(\frac{\pi }{6}-\thetaA\right) \\
\end{smallmatrix}
\right)  & 
\begin{smallmatrix}
0\leq (\theta  \bmod \pi )<\frac{\pi }{6}\lor \\
\frac{\pi }{3}\leq (\theta  \bmod \pi ) < \frac{\pi }{2}\lor \\
\frac{2 \pi }{3}\leq (\theta \bmod \pi ) <\frac{5 \pi }{6} \\
\end{smallmatrix}
\end{cases} 
\end{align}
This can be simplified into 
\begin{align}
\fSe(\theta)  &= 
\left(
\begin{matrix}
 1 \\
 \cos \left(\frac{\pi }{6}-\widetilde{\vartheta}\right) \\
 \cos \left(\frac{\pi }{6}-\widetilde{\theta} \right) \\
\end{matrix}
\right) 
\quad \text{where}  \quad 
\begin{array}{c}
\widetilde{\vartheta} = \left(\theta - \Pii{6}\right) \bmod \frac{\pi }{3} \\
\widetilde{\theta} = \theta  \bmod \frac{\pi }{3} 
\end{array}
\end{align}

then $\fS (\theta) = \fSe (\theta) \otimes \fSs(\theta)$, where $\fSs$ is a piecewise function containing the signs. 

The scaling factors combine with the normalisation scaling factor \ref{eq:L}, found in the previous section, in a simple way
\begin{equation}
\vec{S} = \nS[\theta] \otimes \rS \otimes \fSe (\theta)   = 
\left(
\begin{array}{c}
 \frac{1}{3}  \\
 \frac{1}{2}\\
 \frac{1}{2} \\
\end{array}
\right)
\end{equation}

The fully factorized rotation matrix can now be written as

\begin{align*}\label{eq:TheFactoredRoationMatrix}
\R[\theta] & =  \inS[\theta]\otimes \nS[\theta] \otimes \rS \otimes \fSe (\theta) \otimes \fSs(\theta) \otimes\fRO \left[  \fRm, \theta \right]
\end{align*}

 and so 
 
\begin{align}
\R[\theta] 
& =  \inS[\theta]\otimes \vec{S}  \otimes \fSs(\theta) \otimes\fRO \left[  \fRm, \theta \right] \\
\text{where} \\
\mathbf{L}(\theta) & =
\begin{pmatrix}
\sqrt{3} \\
 \sqrt{\frac{2}{3}} \sin \left(\widetilde{\vartheta}\right) + \sqrt{2} \cos \left(\widetilde{\vartheta}\right) \\  
\sqrt{\frac{2}{3}} \sin \left(\widetilde{\theta}\right) + \sqrt{2} \cos \left(\widetilde{\theta}\right) 
\end{pmatrix}
\quad \text{with{\tiny }}  \quad 
\begin{array}{c}
\widetilde{\vartheta} = \left(\theta - \Pii{6}\right) \bmod \frac{\pi }{3} \\
\widetilde{\theta} = \theta  \bmod \frac{\pi }{3} 
\end{array}
\end{align}

\begin{align}
\fRm
 & = \left(
\begin{smallmatrix}
 1 & 1 & 1 \\
 \text{fRe} (\thetaA)                                        & 1 & \text{fRe} (-\thetaA ) \\
 \text{fRe} \left(\frac{\pi }{6}-\thetaA \right) & \text{fRe} \left(\thetaA -\frac{\pi }{6}\right) & 1 \\
\end{smallmatrix}
\right) 
& \text{with} & \, &
\begin{array}{c}
\fRe[\phi] = \frac{\um 1}{2} \left(1+\sqrt{3} \tan (\phi )\right)  \\
\thetaA = \theta \!\!\mod \Pii{6}
\end{array}
\end{align}


\newcommand{\one}{\quad 1 \quad}
\newcommand{\mOne}{-1 \ }
\begin{align}
\vec{S} & =  
\left(
\begin{smallmatrix}
 \frac{1}{3}  \\
 \frac{1}{2}  \\
 \frac{1}{2}  \\
\end{smallmatrix}
\right) 
&
\fSs[\theta ]   &=
\begin{cases}
 \left(
 \begin{smallmatrix}
  \one \\  \one \\ \one\\
 \end{smallmatrix} 
 \right)  
 & 0\leq (\theta  \bmod \frac{2 \pi }{3}  )<\frac{\pi }{6} \\
 \left(
  \begin{smallmatrix}
   \one \\  \mOne \\ \one\\
  \end{smallmatrix} 
  \right) 
  & \frac{\pi }{6}\leq (\theta  \bmod \frac{2 \pi }{3}  )<\frac{\pi }{3} \\
 \left(
  \begin{smallmatrix}
   \one \\  \mOne \\ \mOne\\
  \end{smallmatrix} 
  \right)
   & \frac{\pi }{3}\leq (\theta  \bmod \frac{2 \pi }{3}  )<\frac{\pi }{2} \\
 \left(
  \begin{smallmatrix}
   \one \\  \one \\ \mOne\\
  \end{smallmatrix} 
  \right) 
  & \frac{\pi }{2}\leq (\theta  \bmod \frac{2 \pi }{3}  )<\frac{2 \pi }{3} 
\end{cases}
\end{align}

\begin{align}
\fRO \left[ 
 \left(\begin{smallmatrix}
 A_1 & A_2 & A_3 \\
 B_1 & B_2 & B_3 \\
 C_1 & C_2 & C_3 \\
\end{smallmatrix} \right), \theta \right]  &=
\begin{cases}
 \left(
\begin{smallmatrix}
 A_1 & A_2 & A_3 \\
 B_1 & B_2 & B_3 \\
 C_1 & C_2 & C_3 \\
\end{smallmatrix}
\right) & 0\leq (\theta  \bmod \pi )<\frac{\pi }{6} \\
 \left(
\begin{smallmatrix}
 A_3 & A_1 & A_2 \\
 C_3 & C_1 & C_2 \\
 B_3 & B_1 & B_2 \\
\end{smallmatrix}
\right) & \frac{\pi }{6}\leq (\theta  \bmod \pi )<\frac{\pi }{3} \\
 \left(
\begin{smallmatrix}
 A_2 & A_3 & A_1 \\
 B_2 & B_3 & B_1 \\
 C_2 & C_3 & C_1 \\
\end{smallmatrix}
\right) & \frac{\pi }{3}\leq (\theta  \bmod \pi )<\frac{\pi }{2} \\
 \left(
\begin{smallmatrix}
 A_1 & A_2 & A_3 \\
 C_1 & C_2 & C_3 \\
 B_1 & B_2 & B_3 \\
\end{smallmatrix}
\right) & \frac{\pi }{2}\leq (\theta  \bmod \pi )<\frac{2 \pi }{3} \\
 \left(
\begin{smallmatrix}
 A_3 & A_1 & A_2 \\
 B_3 & B_1 & B_2 \\
 C_3 & C_1 & C_2 \\
\end{smallmatrix}
\right) & \frac{2 \pi }{3}\leq (\theta  \bmod \pi )<\frac{5 \pi }{6} \\
 \left(
\begin{smallmatrix}
 A_2 & A_3 & A_1 \\
 C_2 & C_3 & C_1 \\
 B_2 & B_3 & B_1 \\
\end{smallmatrix}
\right) & \frac{5 \pi }{6}\leq (\theta  \bmod \pi )<\pi 
\end{cases}
\end{align}

This achieves our goal of factoring the rotation such that the largest element of each matrix row is one.

\section{Quantizing the Rotation}

The factored rotation $\mathbf{fR}$ \ref{eq:TheFactoredRoationMatrix} contains 4 non-integer elements lying between $-1$ and $0$, which we wish to quantize. 

\begin{align}
\R[\theta]  & =  \inS[\theta]\otimes \vec{S}  \otimes \fSs(\theta) \otimes\mathbf{fR} & \textbf{where} &\,&  \mathbf{fR} = \fRO \left[  \fRm, \theta \right]
\end{align}

This can be done simply by multiplying it by $2^{n - 2}$, where $n$ is the bit depth of the integer type, and rounding the result: 

\begin{align}\label{eq:quantizedMatrixDef}
 \qR[\theta, n] &=
\textbf{Round}\left[
\left(
\begin{array}{c}
 1  \\
 2^{n-2} \\
 2^{n-2 }  \\
\end{array}
\right) 
\bigotimes
\fR[\theta] \right] 
\quad &\text{and} \quad 
\qS[n] &= \left(
\begin{array}{c}
 1  \\
 2^{2-n } \\
 2^{2-n }  \\
\end{array}
\right) 
\end{align}

Since every row in $\fR$ always has an element equal to 1, we will exploit the full range $ -2^{n-2} \le \fR \le 2^{n-2}$ of the integer type. 

\begin{align}
\fR[ \theta ] &= \qS[n]  \otimes \left( \qR[ \theta, n]  + \dqR[ \theta, n ] \right)  \\
                  &= \qS[n]  \otimes \qR[ \theta, n ] +  \dfR[ \theta, n ] 
\end{align}

Thus far, we have assumed that we can achieve a bit depth of two less than the bit depth of the source type for the representation of the rotation matrix. We are now in a position to assess whether the representation introduces any errors.

Given that the source type with a bit depth of $n$, we can work out any errors introduced by finding the difference between using the quantized rotation $\qR$ and the unquantized rotation $\fR$. All the factoring of the rotation is mathematically exact up to and including $\fR$, so any errors that are introduced will be during the rounding.

We are interested in the error introduced by the quantization, so we introduce the perturbation $\dfR[\theta]$ associated with the transform $\fR[\theta]$ produced by the rounding. Defining 

\begin{align*}
\fRm[ \theta ] &=  \qS[ n ]  \otimes  \left( \qRm[ \theta, n ]  + \dqRm[ \theta, n ] \right) 
\end{align*}
because $\qS$ has equal elements in the second and third rows
\begin{equation*}
\fRO \left[ \qS[n] \otimes \mathbf{A} \right] = \qS[n] \otimes \fRO[ \mathbf{A} ]  
\end{equation*}
then
\begin{align*}
\fR[ \theta ] &= \fRO \left[ \fRm[ \theta ] \right] \\
                    &= \fRO \left[ \qS[ n ]  \otimes \left( \qRm[ \theta, n ]  + \dqRm[ \theta, n ] \right) \right]  \\
                   &= \qS[ n ]  \otimes \fRO \left[ \qRm[ \theta, n ]  + \dqRm[ \theta, n ]  \right]  \\
                   &= \qS[ n ]  \otimes \fRO \left[ \qRm[ \theta, n ]   \right]  + \qS[ n ]  \otimes \fRO \left[ \dqRm[ \theta, n ]  \right]  
\end{align*}

The perturbation depends entirely on $\dqRm[\theta,n]$ 

\begin{align*}
\dqRe[\phi] &= \text{Round}\left[ 2^{n-2} \fRe[\phi ] \right]-2^{n-2}\fRe[\phi ] \\
&= \text{Round}\left[ 2^{n-2} \frac{\um 1}{2} \left(1+\sqrt{3} \tan (\phi )\right)  \right]-2^{n-2}\frac{\um 1}{2} \left(1+\sqrt{3} \tan (\phi )\right) \\
&= \text{Round}\left[ 2^{n-3} \right]+ \text{Round}\left[ 2^{n-3}\sqrt{3} \tan (\phi ) \right]-2^{n-3}-2^{n-3}\sqrt{3} \tan (\phi )\\
&=  \text{Round}\left[ 2^{n-3}\sqrt{3} \tan (\phi ) \right]-2^{n-3}\sqrt{3} \tan (\phi )\\
\dfRe[\phi] &= 2^{2-n} \dqRe[\phi]  
\end{align*}

The perturbation $\dqR[\theta]$ is a rearrangement of $\dqRm = \dqR[\thetaA] $  where $\thetaA = \theta \bmod \frac{\pi}{6}$
\begin{align*}
\dqR[\theta] &= \fRO[\dqRm, \theta ] \\
 &=
 \fRO \left[
\begin{pmatrix}
 0                                     & 0                                   & 0                     \\
 \dqRe[ \thetaA ]              & 0                                   & \dqRe[ -\thetaA ] \\
 \dqRe[ \Pii{6} -\thetaA]   & \dqRe[\thetaA-\Pii{6} ] & 0                     \\
\end{pmatrix}, \theta \right]
\end{align*}

The perturbation introduced to each channel is further simplified by recognizing that $\fRe[-\phi]=-1-\fRe[\phi]$, which allows us to find $\dqRe[-\phi]=-\dqRe[\phi]$ i.e. that the sum of the two perturbations in each row equals zero. we can analyse the quantization error in terms of just one function $\dqRe$ and a matrix ordering function $\dqR[\theta] = \fRO \left[ \dqR[ \thetaA ], \theta  \right]$.

\begin{align*}
\dqR[\theta] &= \fROm \left[  
\begin{pmatrix}
 0                                     \\
 \dqRe[ \thetaA ]              \\
 \dqRe[ \Pii{6} -\thetaA]   \\
\end{pmatrix} \otimes
\begin{pmatrix}
 0  &    0  &   0  \\
 1  &    0  & -1  \\
 1  & - 1  &   0  \\
\end{pmatrix}
  ,\theta \right] \\
  &= \dqEO \left[ 
  \begin{pmatrix}
   0                                     \\
   \dqRe[ \thetaA ]              \\
   \dqRe[ \Pii{6} -\thetaA]   \\
  \end{pmatrix} 
  \right]
  \otimes
  \dqs[\theta] 
\end{align*}

where
%\begin{equation}
%\fROm \left[  \left(
%\begin{smallmatrix}
% 0   & 0   & 0  \\
% a   & 0   & -a \\
% b & - b & 0    \\
% \end{smallmatrix} 
%\right)  ,\theta \right] =
%\begin{cases}
% \left(
%\begin{smallmatrix}
% 0 & 0 & 0 \\
% a & 0 & -a \\
% b & -b & 0 \\
%\end{smallmatrix} 
%\right) & 0\leq (\theta  \bmod \pi )<\frac{\pi }{6} \\
% \left(
%\begin{smallmatrix}
% 0 & 0 & 0 \\
% 0 & b & -b \\
% -a & a & 0 \\
%\end{smallmatrix} 
%\right) & \frac{\pi }{6}\leq (\theta  \bmod \pi )<\frac{\pi }{3} \\
% \left(
%\begin{smallmatrix}
% 0 & 0 & 0 \\
% 0 & -a & a \\
% -b & 0 & b \\
%\end{smallmatrix} 
%\right) & \frac{\pi }{3}\leq (\theta  \bmod \pi )<\frac{\pi }{2} \\
% \left(
%\begin{smallmatrix}
% 0 & 0 & 0 \\
% b & -b & 0 \\
% a & 0 & -a \\
%\end{smallmatrix} 
%\right) & \frac{\pi }{2}\leq (\theta  \bmod \pi )<\frac{2 \pi }{3} \\
% \left(
%\begin{smallmatrix}
% 0 & 0 & 0 \\
% -a & a & 0 \\
% 0 & b & -b \\
%\end{smallmatrix} 
%\right) & \frac{2 \pi }{3}\leq (\theta  \bmod \pi )<\frac{5 \pi }{6} \\
% \left(
%\begin{smallmatrix}
% 0 & 0 & 0 \\
% -b & 0 & b \\
% 0 & -a & a \\
%\end{smallmatrix} 
%\right) & \frac{5 \pi }{6}\leq (\theta  \bmod \pi )<\pi 
%\end{cases}
%\end{equation}


\begin{equation}
\dqEO[  \left(
  \begin{smallmatrix} 0   \\ a   \\ b  \\ \end{smallmatrix} 
\right)  ,\theta ] =
\begin{cases}
 \left(
  \begin{smallmatrix} 0 \\ a  \\ b \\
\end{smallmatrix} 
\right) & 0\leq (\theta  \bmod \frac{\pi}{3} )<\frac{\pi }{6} \\
 \left(
  \begin{smallmatrix} 0 \\ b  \\ a \\ \end{smallmatrix} 
\right) & \frac{\pi }{6} \leq (\theta  \bmod \frac{\pi}{3} ) < \frac{\pi }{3} 
\end{cases}
\end{equation}

\begin{equation}\label{eq:perturbationFunctionSigns}
\dqs[\theta] =
\begin{cases}
 \left(
\begin{smallmatrix}
 0 & 0 & 0 \\
 1 & 0 & -1 \\
 1 & -1 & 0 \\
\end{smallmatrix} 
\right) & 0\leq (\theta  \bmod \pi )<\frac{\pi }{6} \\
 \left(
\begin{smallmatrix}
 0 & 0 & 0 \\
 0 & 1 & -1 \\
 -1 & 1 & 0 \\
\end{smallmatrix} 
\right) & \frac{\pi }{6}\leq (\theta  \bmod \pi )<\frac{\pi }{3} \\
 \left(
\begin{smallmatrix}
 0 & 0 & 0 \\
 0 & -1 & 1 \\
 -1 & 0 & 1 \\
\end{smallmatrix} 
\right) & \frac{\pi }{3}\leq (\theta  \bmod \pi )<\frac{\pi }{2} \\
 \left(
\begin{smallmatrix}
 0 & 0 & 0 \\
 1 & -1 & 0 \\
 1 & 0 & -1 \\
\end{smallmatrix} 
\right) & \frac{\pi }{2}\leq (\theta  \bmod \pi )<\frac{2 \pi }{3} \\
 \left(
\begin{smallmatrix}
 0 & 0 & 0 \\
 -1 & 1 & 0 \\
 0 & 1 & -1 \\
\end{smallmatrix} 
\right) & \frac{2 \pi }{3}\leq (\theta  \bmod \pi )<\frac{5 \pi }{6} \\
 \left(
\begin{smallmatrix}
 0 & 0 & 0 \\
 -1 & 0 & 1 \\
 0 & -1 & 1 \\
\end{smallmatrix} 
\right) & \frac{5 \pi }{6}\leq (\theta  \bmod \pi )<\pi 
\end{cases}
\end{equation}

We can now express the perturbation to the rotation and to the channel elements themselves. The structure of $\fROm$ means that the perturbation to each channel is the product of either $\dqRe[ \thetaA ]$ or $\dqRe[ \Pii{6} -\thetaA]$ with the difference between two of the pixel values. Given that we can assume that the pixel values are positive, being unsigned integers, the extreme values of the perturbation are when one of the two pixel values is zero.

%\begin{align}
%\nR[\theta]  & = \vec{S}  \otimes \fSs(\theta) \otimes \fRO \left[  \fRm, \theta \right] \\
%                   & = \vec{S}  \otimes \fSs(\theta) \otimes\qS[ n ] \otimes \fRO \left[ \qRm[ \theta, n ]  + \dqRm[ \theta, n ]  , \theta\right]
%\end{align}

\begin{align}
\nR[\theta] & = \nRa[\theta]+\dnR[\theta] \\ 
\textbf{where}  \\
\nRa[\theta]   &=  \vec{S}  \otimes \fSs(\theta) \otimes\qS[ n ] \otimes \fRO \left[   \qRm[ \theta, n ] , \theta\right]   \\
% \dnR[\theta]   &=  \vec{S}  \otimes \fSs(\theta) \otimes\qS[ n ] \otimes \fRO \left[ \dqRm[ \theta, n ] , \theta\right]  \\
\dnR[\theta]   &=  \vec{S}  \otimes \fSs(\theta) \otimes\qS[ n ] \otimes 
\dqEO \left[ \begin{smallmatrix}  0 \\  \dqRe[ \thetaA ]  \\  \dqRe[ \Pii{6} -\thetaA]   \\ \end{smallmatrix}  \right] \otimes
 \dqs[\theta]  \label{eq:dnR_Expanded_and_Quantised}
\end{align}
 

the perturbation to the rotated channel elements $\dW$ is found for an input set of pixel values $\rgb= 2^n\normed{\rgb} $ where $0 \le \normed{\rgb}  \le 1$

\begin{align*}
\w+\dW &= \nRa[\theta] \cdot \rgb+\dnR[\theta] \cdot \rgb \\
\dW &= 2^n \; \dnR[\theta] \cdot \normed{\rgb} \\
\end{align*}

Expanding $\dnR[\theta] $ using \ref{eq:dnR_Expanded_and_Quantised}.

\begin{align*}
\dW  &= 2^n \; \vec{S}  \otimes\qS[ n ]  \otimes 
\fSs(\theta) \otimes 
\dqEO \left[ \begin{smallmatrix} 0   \\ \dqRe[ \thetaA ]  \\ \dqRe[ \Pii{6} -\thetaA]  \end{smallmatrix} \right] \otimes 
\dqs[\theta]  \cdot \normed{\rgb}  
\\
&= \left(
\begin{smallmatrix}
\frac{2^n}{3}  \\  2 \\  2 \\
\end{smallmatrix}
\right)  \otimes \dqEO \left[ 
\begin{smallmatrix}
0                                     \\
\dqRe[ \thetaA ]              \\
\dqRe[ \Pii{6} -\thetaA]   
\end{smallmatrix} 
\right] \otimes \fSs(\theta)  \otimes \dqs[\theta] \cdot \normed{\rgb} \\
&= \dqEO \left[ 
\begin{smallmatrix}
 0                                     \\
 2 \dqRe[ \thetaA ]              \\
 2 \dqRe[ \Pii{6} -\thetaA]   
\end{smallmatrix} 
\right] \otimes \fSs(\theta)  \otimes \dqs[\theta] \cdot \normed{\rgb} \\
\end{align*}


%   % Checks for the scaling products.
%\begin{align*}
%2^n \, \vec{S}  \otimes \qS[n] 
% &= 2^n \, \left( \begin{smallmatrix}  \frac{1}{3}  \\  \frac{1}{2}  \\  \frac{1}{2}  \\ \end{smallmatrix} \right)  \otimes 
% \left( \begin{smallmatrix} 1  \\  2^{2-n } \\  2^{2-n }  \\ \end{smallmatrix} \right)  \\ 
%&= \left( \begin{smallmatrix} \frac{2^n}{3}  \\ 2 \\ 2 \\ \end{smallmatrix} \right)
%\end{align*}
%
%
%\begin{align}
%\fSs[\theta ]   &=
%\begin{cases}
% \left(
% \begin{smallmatrix}
%  \one \\  \one \\ \one\\
% \end{smallmatrix} 
% \right)  
% & 0\leq (\theta  \bmod \frac{2 \pi }{3}  )<\frac{\pi }{6} \\
% \left(
%  \begin{smallmatrix}
%   \one \\  \mOne \\ \one\\
%  \end{smallmatrix} 
%  \right) 
%  & \frac{\pi }{6}\leq (\theta  \bmod \frac{2 \pi }{3}  )<\frac{\pi }{3} \\
% \left(
%  \begin{smallmatrix}
%   \one \\  \mOne \\ \mOne\\
%  \end{smallmatrix} 
%  \right)
%   & \frac{\pi }{3}\leq (\theta  \bmod \frac{2 \pi }{3}  )<\frac{\pi }{2} \\
% \left(
%  \begin{smallmatrix}
%   \one \\  \one \\ \mOne\\
%  \end{smallmatrix} 
%  \right) 
%  & \frac{\pi }{2}\leq (\theta  \bmod \frac{2 \pi }{3}  )<\frac{2 \pi }{3} 
%\end{cases} \\
%\dqs[\theta] &=
%\begin{cases}
% \left(
%\begin{smallmatrix}
% 0 & 0 & 0 \\
% 1 & 0 & -1 \\
% 1 & -1 & 0 \\
%\end{smallmatrix} 
%\right) & 0\leq (\theta  \bmod \pi )<\frac{\pi }{6} \\
% \left(
%\begin{smallmatrix}
% 0 & 0 & 0 \\
% 0 & 1 & -1 \\
% -1 & 1 & 0 \\
%\end{smallmatrix} 
%\right) & \frac{\pi }{6}\leq (\theta  \bmod \pi )<\frac{\pi }{3} \\
% \left(
%\begin{smallmatrix}
% 0 & 0 & 0 \\
% 0 & -1 & 1 \\
% -1 & 0 & 1 \\
%\end{smallmatrix} 
%\right) & \frac{\pi }{3}\leq (\theta  \bmod \pi )<\frac{\pi }{2} \\
% \left(
%\begin{smallmatrix}
% 0 & 0 & 0 \\
% 1 & -1 & 0 \\
% 1 & 0 & -1 \\
%\end{smallmatrix} 
%\right) & \frac{\pi }{2}\leq (\theta  \bmod \pi )<\frac{2 \pi }{3} \\
% \left(
%\begin{smallmatrix}
% 0 & 0 & 0 \\
% -1 & 1 & 0 \\
% 0 & 1 & -1 \\
%\end{smallmatrix} 
%\right) & \frac{2 \pi }{3}\leq (\theta  \bmod \pi )<\frac{5 \pi }{6} \\
% \left(
%\begin{smallmatrix}
% 0 & 0 & 0 \\
% -1 & 0 & 1 \\
% 0 & -1 & 1 \\
%\end{smallmatrix} 
%\right) & \frac{5 \pi }{6}\leq (\theta  \bmod \pi )<\pi 
%\end{cases} \\
%\end{align}
%
%\begin{align}
%\fSs[\theta ]  \otimes \dqs[\theta] &=
%\begin{cases}
%\left(
% \begin{smallmatrix}
%  \one \\  \one \\ \one\\
% \end{smallmatrix} 
% \right)  \otimes 
% \left(
%\begin{smallmatrix}
% 0 & 0 & 0 \\
% 1 & 0 & -1 \\
% 1 & -1 & 0 \\
%\end{smallmatrix} 
%\right) & 0\leq \theta<\frac{\pi }{6} \\
%\left(
%  \begin{smallmatrix}
%   \one \\  \mOne \\ \one\\
%  \end{smallmatrix} 
%  \right) \otimes \left(
%\begin{smallmatrix}
% 0 & 0 & 0 \\
% 0 & 1 & -1 \\
% -1 & 1 & 0 \\
%\end{smallmatrix} 
%\right) & \frac{\pi }{6}\leq \theta<\frac{\pi }{3} \\
% \left(
%   \begin{smallmatrix}
%    \one \\  \mOne \\ \mOne\\
%   \end{smallmatrix} 
%   \right) \otimes \left(
%\begin{smallmatrix}
% 0 & 0 & 0 \\
% 0 & -1 & 1 \\
% -1 & 0 & 1 \\
%\end{smallmatrix} 
%\right) & \frac{\pi }{3}\leq \theta<\frac{\pi }{2} \\
% \left(
%   \begin{smallmatrix}
%    \one \\  \one \\ \mOne\\
%   \end{smallmatrix} 
%   \right) \otimes 
%\left(
%\begin{smallmatrix}
% 0 & 0 & 0 \\
% 1 & -1 & 0 \\
% 1 & 0 & -1 \\
%\end{smallmatrix} 
%\right) & \frac{\pi }{2}\leq \theta<\frac{2 \pi }{3} \\
%%
%\left(
% \begin{smallmatrix}
%  \one \\  \one \\ \one\\
% \end{smallmatrix} 
% \right)  \otimes 
%\left(
%\begin{smallmatrix}
% 0 & 0 & 0 \\
% -1 & 1 & 0 \\
% 0 & 1 & -1 \\
%\end{smallmatrix} 
%\right) & \frac{2 \pi }{3}\leq \theta<\frac{5 \pi }{6} \\
% \left(
%   \begin{smallmatrix}
%    \one \\  \mOne \\ \one\\
%   \end{smallmatrix} 
%   \right) \otimes 
%   \left(
%\begin{smallmatrix}
% 0 & 0 & 0 \\
% -1 & 0 & 1 \\
% 0 & -1 & 1 \\
%\end{smallmatrix} 
%\right) & \frac{5 \pi }{6}\leq \theta<\pi \\
%\left(
%   \begin{smallmatrix}
%    \one \\  \mOne \\ \mOne\\
%   \end{smallmatrix} 
%   \right) \otimes 
%   \left(
%\begin{smallmatrix}
% 0 & 0 & 0 \\
% 1 & 0 & -1 \\
% 1 & -1 & 0 \\
%\end{smallmatrix} 
%\right) & \pi \leq \theta<\frac{7 \pi }{6} \\
% \left(
%    \begin{smallmatrix}
%     \one \\  \one \\ \mOne\\
%    \end{smallmatrix} 
%    \right) \otimes 
%    \left(
%\begin{smallmatrix}
% 0 & 0 & 0 \\
% 0 & 1 & -1 \\
% -1 & 1 & 0 \\
%\end{smallmatrix} 
%\right) & \frac{7 \pi }{6}\leq \theta<\frac{8 \pi }{6} \\
% \left(
%  \begin{smallmatrix}
%   \one \\  \one \\ \one\\
%  \end{smallmatrix} 
%  \right)  \otimes 
%  \left(
%\begin{smallmatrix}
% 0 & 0 & 0 \\
% 0 & -1 & 1 \\
% -1 & 0 & 1 \\
%\end{smallmatrix} 
%\right) & \frac{8 \pi }{6}\leq \theta<\frac{9 \pi }{6} \\
% \left(
%   \begin{smallmatrix}
%    \one \\  \mOne \\ \one\\
%   \end{smallmatrix} 
%   \right) \otimes \left(
%\begin{smallmatrix}
% 0 & 0 & 0 \\
% 1 & -1 & 0 \\
% 1 & 0 & -1 \\
%\end{smallmatrix} 
%\right) & \frac{9 \pi }{6}\leq \theta<\frac{10 \pi }{6} \\
% \left(
%    \begin{smallmatrix}
%     \one \\  \mOne \\ \mOne\\
%    \end{smallmatrix} 
%    \right) \otimes 
%    \left(
%\begin{smallmatrix}
% 0 & 0 & 0 \\
% -1 & 1 & 0 \\
% 0 & 1 & -1 \\
%\end{smallmatrix} 
%\right) & \frac{10 \pi }{6}\leq \theta<\frac{11 \pi }{6} \\
%\left(
%   \begin{smallmatrix}
%    \one \\  \one \\ \mOne\\
%   \end{smallmatrix} 
%   \right) \otimes \left(
%\begin{smallmatrix}
% 0 & 0 & 0 \\
% -1 & 0 & 1 \\
% 0 & -1 & 1 \\
%\end{smallmatrix} 
%\right) & \frac{11 \pi }{6}\leq \theta< 2 \pi 
%\end{cases} 
%\end{align}
%
%\begin{align}
%\fSs[\theta ]  \otimes \dqs[\theta] &=
%\begin{cases}
% \left(
%\begin{smallmatrix}
% 0 & 0 & 0 \\
% 1 & 0 & -1 \\
% 1 & -1 & 0 \\
%\end{smallmatrix} 
%\right) & 0\leq \theta<\frac{\pi }{6} \\
%\left(
%\begin{smallmatrix}
% 0 & 0 & 0 \\
% 0 & -1 & 1 \\
% -1 & 1 & 0 \\
%\end{smallmatrix} 
%\right) & \frac{\pi }{6}\leq \theta<\frac{\pi }{3} \\
% \left(
%\begin{smallmatrix}
% 0 & 0 & 0 \\
% 0 & 1 & -1 \\
% 1 & 0 & -1 \\
%\end{smallmatrix} 
%\right) & \frac{\pi }{3}\leq \theta<\frac{\pi }{2} \\
%\left(
%\begin{smallmatrix}
% 0 & 0 & 0 \\
% 1 & -1 & 0 \\
% -1 & 0 & 1 \\
%\end{smallmatrix} 
%\right) & \frac{\pi }{2}\leq \theta<\frac{2 \pi }{3} \\
%\left(
%\begin{smallmatrix}
% 0 & 0 & 0 \\
% -1 & 1 & 0 \\
% 0 & 1 & -1 \\
%\end{smallmatrix} 
%\right) & \frac{2 \pi }{3}\leq \theta<\frac{5 \pi }{6} \\
%   \left(
%\begin{smallmatrix}
% 0 & 0 & 0 \\
% 1 & 0 & -1 \\
% 0 & -1 & 1 \\
%\end{smallmatrix} 
%\right) & \frac{5 \pi }{6}\leq \theta<\pi \\ 
%   \left(
%\begin{smallmatrix}
% 0 & 0 & 0 \\
% -1 & 0 & 1 \\
% -1 & 1 & 0 \\
%\end{smallmatrix} 
%\right) & \pi \leq \theta<\frac{7 \pi }{6} \\
%    \left(
%\begin{smallmatrix}
% 0 & 0 & 0 \\
% 0 & 1 & -1 \\
% 1 & -1 & 0 \\
%\end{smallmatrix} 
%\right) & \frac{7 \pi }{6}\leq \theta<\frac{8 \pi }{6} \\ 
%  \left(
%\begin{smallmatrix}
% 0 & 0 & 0 \\
% 0 & -1 & 1 \\
% -1 & 0 & 1 \\
%\end{smallmatrix} 
%\right) & \frac{8 \pi }{6}\leq \theta<\frac{9 \pi }{6} \\
% \left(
%\begin{smallmatrix}
% 0 & 0 & 0 \\
% -1 & 1 & 0 \\
% 1 & 0 & -1 \\
%\end{smallmatrix} 
%\right) & \frac{9 \pi }{6}\leq \theta<\frac{10 \pi }{6} \\
%    \left(
%\begin{smallmatrix}
% 0 & 0 & 0 \\
% 1 & -1 & 0 \\
% 0 & -1 & 1 \\
%\end{smallmatrix} 
%\right) & \frac{10 \pi }{6}\leq \theta<\frac{11 \pi }{6} \\
%\left(
%\begin{smallmatrix}
% 0 & 0 & 0 \\
% -1 & 0 & 1 \\
% 0 & 1 & -1 \\
%\end{smallmatrix} 
%\right) & \frac{11 \pi }{6}\leq \theta< 2 \pi 
%\end{cases} 
%\end{align}


So, to minimize the perturbation, we need to simultaneously minimize $\pm 2 \dqRe[ \thetaA ]  $ and $\pm 2 \dqRe[ \Pii{6} -\thetaA]$


%\begin{figure}[h!]
%  \centering
%    \includegraphics[width=0.99\textwidth]{Chapter2/Figs/PtbToChanBoth.jpg}
%    \caption{Perturbation in the chromatic channels.}  \label{fig:PtbToChan}
%\end{figure}


\begin{sidewaysfigure}[p]
  \centering
    \includegraphics[width=0.99\textwidth]{Chapter2/Figs/PtbToChanBothWithPoints.jpg}
    \caption{ Perturbation to channels a and b against angle $\thetaA$ \\
    For n = 8  with $\iota$ regions shaded and extrema labelled }  \label{fig:PtbToChan}
\end{sidewaysfigure}

The maxima for each function are where $\pm 2 \dqRe[ \thetaA ]  = \pm 1$ and $\pm 2 \dqRe[ \Pii{6} -\thetaA]= \pm 1$, and the minima are where $\pm 2 \dqRe[ \thetaA ]  = 0$ and $\pm 2 \dqRe[ \Pii{6} -\thetaA] = 0$. To find the corresponding values of $\thetaA$, all that is needed is an inverse function for $\dqRe$. In essence $\dqRe[\phi]$ discretizes $\sqrt{3} \tan (\phi )$ into steps of $2^{3-n}$. The minima can be found by taking the inverse of $\sqrt{3} \tan (\phi )$, given that
$0\le\sqrt{3} \tan (\phi )\le1$ for $0 \le \phi \le \Pii{6}$ we can say that $\text{Round}\left[ 2^{n-3}\sqrt{3} \tan (\phi ) \right] \in \{ 0, 1, 2, \cdots 2^{n-3} \}$ 


\begin{align*}
\dqRe[\phi] & = 0 & \dqRe[\Pii{6}-\phi] & = 0 \quad \text{when} \\
% 2^{n-3}\sqrt{3} \tan (\phi ) &= \text{Round}\left[ 2^{n-3}\sqrt{3} \tan (\phi ) \right]  & 
% 2^{n-3}\sqrt{3} \tan (\Pii{6}-\phi ) &= \text{Round}\left[ 2^{n-3}\sqrt{3} \tan (\Pii{6}-\phi ) \right]\\
2^{n-3}\sqrt{3} \tan (\phi ) & = m & 
2^{n-3}\sqrt{3} \tan (\Pii{6}-\phi ) &= m \quad \text{where}\quad m=0,1,2 \cdots 2^{n-3} \\
 \minimaTheta{b} = & \arctan\left(\frac{m 2^{3-n}}{\sqrt{3}}  \right) &
 \minimaTheta{a} = & \arctan\left(\frac{2 \sqrt{3} m}{2^n - 2 m}\right) 
\end{align*}

The maxima can be found by recognizing that the maximum rounding error is a half and evaluating at these points

\begin{align*}
\dqRe[\maximaTheta{b} ] & = \pm 1 & \dqRe[\Pii{6}-\maximaTheta{a} ] & = \pm 1  \quad \text{when} \\
\maximaTheta{b} & =\arctan\left(\frac{ 2^{3-n} }{\sqrt{3}} \left(m+\frac{1}{2}\right) \right) &
\maximaTheta{a} & = \arctan\left(\frac{2 \sqrt{3} \left(m+\frac{1}{2}\right)}{2^n - 2 \left(m+\frac{1}{2}\right)}\right)  \\
 \maximaTheta{b}  &=  \arctan\left(\frac{(2 m+1) 2^{2-n}}{\sqrt{3}}          \right) &
  \maximaTheta{a}  &=  \arctan\left(\frac{\sqrt{3} (2 m+1)}{2^n-2 m-1}  \right) \\
  \quad&\text{with}\quad   m =  0,1,2,3 \cdots2^{n-3}-1 
\end{align*}

We are interested in a compromise between the perturbations to each channel which is where the perturbations intersect. It would be useful to be able to assign a relative importance $\alpha$ and $\beta$ to each channel a and b respectively. 

\begin{gather*}
\beta \; \dqRe[\phi] = \alpha \; \dqRe[\frac{\pi}{6} -\phi]  \quad \textbf{when} \\
\begin{split}
\beta \text{Round}\left[ 2^{n-3}\sqrt{3} \tan (\phi ) \right] - \alpha \text{Round}\left[ 2^{n-3}\sqrt{3} \tan (\frac{\pi}{6} - \phi ) \right] \\
=   2^{n-3} \left(\sqrt{3} \alpha  \tan (\theta ) + 2 \beta  \sin (\theta ) \sec \left(\frac{\pi }{6}-\theta \right)\right)
\end{split}
\end{gather*}

The values $\alpha$ and $\beta$ can only move the point of intersection within the bounds of the extrema. Therefore, any solution for the point of intersection will enable the bounding extrema to be identified. 

We solve first for the case where $\alpha=1$ and $\beta=1$. Recognizing that the rounded part can only take certain values
\begin{equation}
i =  \text{Round}\left[ 2^{n-3}\sqrt{3} \tan (\phi ) \right] - \text{Round}\left[ 2^{n-3}\sqrt{3} \tan (\frac{\pi}{6} - \phi ) \right]
 \quad \text{where}\quad i=0,1,2 \cdots 2^{n-2}
 \end{equation}
then


\begin{gather*}
\thetaX{i}=\arctan\left(\frac{\sqrt{i^2 2^{6-2 n}-i 2^{4-n}+49}+i 2^{3-n}-7}{2 \sqrt{3}}\right) \quad \text{where}\quad i=0,1,2 \cdots 2^{n-2}
\end{gather*}

Each point of intersection is defined and bounded by 4 extrema. Comparing with the functional form of the extrema allows these 4 points to be identified.
%
%\begin{align*}
% \minimaTheta{a} = & \arctan\left(\frac{2m \sqrt{3} }{2^n - 2 m}\right) &
% \minimaTheta{b} = & \arctan\left(\frac{2m 2^{2-n}}{\sqrt{3}}       \right) \\ 
% & \text{with}\quad   m =  0,1,2,3 \cdots2^{n-3} \\
% \hat{\phi    }_{a}  = & \arctan\left(\frac{\sqrt{3} (2 m+1)}{2^n-(2 m+1)}  \right) &
% \hat{\phi   }_{b}  = & \arctan\left(\frac{(2 m+1) 2^{2-n}}{\sqrt{3}}          \right) \\
% &\text{with}\quad   m =  0,1,2,3 \cdots2^{n-3}-1 
%\end{align*}
%


\begin{align}\label{eq:ExtremaFunctions}
 \extremaTheta{a} {l}= & \arctan\left(\frac{l \sqrt{3} }{2^n - l}\right) &
 \extremaTheta{b} {l}= & \arctan\left(\frac{l \; 2^{2-n}}{\sqrt{3}}       \right) \\ 
 \minimaTheta{a} = & \extremaTheta{a} {2m} &
 \minimaTheta{b} = & \extremaTheta{b} {2m} 
 & \text{with}\quad   m =  0,1,2,3 \cdots2^{n-3} \\
  \maximaTheta{a} = & \extremaTheta{a} {2m+1} &
  \maximaTheta{b} = & \extremaTheta{b} {2m+1} 
 &\text{with}\quad   m =  0,1,2,3 \cdots2^{n-3}-1 
\end{align}

%\begin{tabular}{|c|c|c|c|}
%\hline
%  & Channel a & Channel b &  m \\ 
%\hline Minima  &  $ \minimaTheta{a}(m)  =  \extremaTheta{a}{2m}  $ & $ \minimaTheta{b}(m)  = \extremaTheta{b}{2m}  $ &  $m \in \{ 0,1,2,3 \cdots2^{n-3}  \} $\\ 
%\hline Maxima  &  $ \maximaTheta{a}(m)  = \extremaTheta{a}{2m+1}   $ & $ \maximaTheta{b}(m)  = \extremaTheta{b}{2m+1}  $  & $ m \in \{ 0,1,2,3 \cdots2^{n-3} -1 \}  $ \\ 
%\hline 
%\end{tabular} 

Separating the points of intersection into even and odd terms facilitates the identification. 

\begin{tabular}{|c|c|c|c|}
\hline
  & Channel a & Channel b &  i \\ 
\hline Minima  &  $ \minimaTheta{a}(\frac{i}{2})=\extremaTheta{a}{i} $ & $ \minimaTheta{b}(\frac{i}{2})=\extremaTheta{b}{i} $ & $ i \in \{0,2,4,6\cdots2^{n-2} \} $\\ 
\hline Maxima  &  $     \maximaTheta{a}(\frac{i-1}{2})=\extremaTheta{a}{i} $ & $     \maximaTheta{b}(\frac{i-1}{2})=\extremaTheta{b}{i} $ & $ i \in \{1,3,5,7\cdots2^{n-2}-1\}  $ \\ 
\hline 
\end{tabular} 


The following ranges are guaranteed to be true for even or odd values of $i$, with very specific values of $p$ and $q$.
%
%\begin{align*}
%\begin{array}{cc}
%  \extremaTheta{b}{p} & < \\
%  \extremaTheta{a} {q-1} & < 
%\end{array}
%&\thetaX{i} 
%\begin{array}{cc}
%  < & \extremaTheta{a}{q}  \\
%  < & \extremaTheta{b}{p+1}  
%\end{array} 
%& \bigvee
%\begin{array}{cc}
%  \extremaTheta{a}{q} & < \\
%  \extremaTheta{b} {p+1} & < 
%\end{array}
%&\thetaX{i+1}
%\begin{array}{cc}
%  < & \extremaTheta{b}{p+2}  \\
%  < & \extremaTheta{a}{q+1}  
%\end{array} & 
%\begin{array}{cc}
%p &\in \{ 2 \mathbb{N} +1 \}  \\
%q &\in \{ 2 \mathbb{N} +1 \}  \\
%i  &\in \{ 2 \mathbb{N} +1 \}
%\end{array}
%\end{align*}


\begin{align*}
\begin{array}{cc}
  \extremaTheta{b}{p-1} & < \\
  \extremaTheta{a} {q} & < 
\end{array}
&\thetaX{i} 
\begin{array}{cc}
  < & \extremaTheta{a}{q+1}  \\
  < & \extremaTheta{b}{p}  
\end{array} 
& \bigvee
\begin{array}{cc}
  \extremaTheta{a}{q+1} & < \\
  \extremaTheta{b} {p} & < 
\end{array}
&\thetaX{i+1} 
\begin{array}{cc}
  < & \extremaTheta{b}{p+1}  \\
  < & \extremaTheta{a}{q+2}  
\end{array} & 
\begin{array}{cc}
p  &\in \{ 2 \mathbb{N}  \}  \\
q  &\in \{ 2 \mathbb{N}  \}  \\
2  & \le i  \le 2^{n-2} -1
\end{array}
\end{align*}


This slightly glib definition illustrates the pattern for the extrema bounding the points of intersection. At the start of a $\frac{\Pi}{6}$ region, i.e. for low values of $i$, $p=i$ and $q=i$. The maxima swap orderings between certain values of $i$ within a $\frac{\Pi}{6}$ region. This is evident in the inescapable indices of $i-1$ and $i+1$, regardless of the starting values for $i$ for either the maxima or minima. So, whilst it is true that $\extremaTheta{a} {i} > \extremaTheta{b} {i}$, it is not always true that $\extremaTheta{a} {i+1} > \extremaTheta{b} {i-1}$. This leads to the question of what happens when this boundary $\extremaTheta{a} {i+1} = \extremaTheta{b} {i-1}$ is crossed. Firstly, the even-odd ordering (i.e. the a-b channel ordering) is flipped; secondly, the indices are shifted. A second transition restores the a-b ordering and shifts the indices again. As the index $i$ passes the half way point $\meanIndx = 2^{n-3}$, the transitions occur in the opposite direction, restoring the indices and ordering. Transitions occur where $i$ satisfies $\extremaTheta{a} {i+\iota} = \extremaTheta{b} {i-\iota}$, with $\iota \in \{ 1,2,3 \cdots \left(7-4 \sqrt{3}\right) 2^{n-3}\} $. 

\begin{gather*}
\meanIndx = 2^{n-3} \qquad
\varGamma(\iota, n) = \sqrt{\iota ^2 - 14 \iota \meanIndx + \meanIndx^{2} } 
\end{gather*}

The region $\iota$ for a given value of $i$ is the highest value of $\iota$ for which the following condition remains true:

\begin{gather*}
\meanIndx -\varGamma(\iota, n) \leq i \leq \varGamma(\iota, n) + \meanIndx 
\end{gather*}

For a given value of $\theta$ and $n$, we can find the intersection index $i$, and from this we can find $\iota$:

\begin{gather}\label{eq:indexFromTheta}
i(\thetaA, n) = \frac{\meanIndx \tan (\thetaA ) \left(\sqrt{3} \tan (\thetaA )+7\right)}{\tan (\thetaA )+\sqrt{3}} 
\quad \text{where} \quad \thetaA = \theta \bmod \frac{\pi}{6}  \\
 \iota(i,n)  = \left\lfloor 7\ \meanIndx-\sqrt{i^2 - 2 i \; \meanIndx + 49\ \meanIndx^2 }\right\rfloor 
\end{gather}

The sequence of a-b channel extrema depends on both the index $i$ and the region $\iota$ in the following way:

\begin{align}\label{eq:extremaBounds}
\begin{array}{cc}
  \extremaTheta{b}{i-\iota-1} & < \\
  \extremaTheta{a} {i+\iota} & < 
\end{array}
&\thetaX{i}
\begin{array}{cc}
  < &  \extremaTheta{a}{i+\iota+1}  \\
  < & \extremaTheta{b}{i-\iota}  
\end{array} & \text{when} \quad 
\begin{array}{lcl}
i \in \{2 \mathbb{N}+1\}  & \land & \iota \in \{2 \mathbb{N}+1\} \  \lor \\ 
i \in \{2 \mathbb{N}     \} &  \land & \iota \in \{2 \mathbb{N}    \} \\
 \end{array}  \\
\begin{array}{cc}
  \extremaTheta{a}{i+\iota} & < \\
  \extremaTheta{b} {i-\iota-1} & < 
\end{array}
&\thetaX{i}
\begin{array}{cc}
  < & \extremaTheta{b}{i-\iota}  \\
  < & \extremaTheta{a}{i+\iota+1}  
\end{array} & 
\text{when} \quad 
\begin{array}{lcl}
 (i\in \{2 \mathbb{N}+1\} & \land & \iota \in \{2 \mathbb{N}    \}) \ \lor \\
 (i\in \{2 \mathbb{N}    \} & \land & \iota \in \{2 \mathbb{N}+1\})
 \end{array} 
\end{align}

The other two ranges can swap direction around the central value  $\overline{m}$ in a region governed by $\Gamma$ which is the distance from $\overline{m}$ at which the order of the extrema switch between channels.

\newcommand{\lessGtrOp}[1]{ \underset{ \mbox{\small$#1$} }{ \mbox{\Huge$\lessgtr$}  }}

\begin{gather*}
\overline{m}=\frac{1}{2} \left(2^{n-3}-1\right) \qquad
\varGamma(n)=\frac{1}{2} \sqrt{2^{2 (n-3)}-14\ 2^{n-3}+1} \\
\lessGtrOp{m} =\begin{cases}
< & \quad    m <   \bar{m} -\Gamma \; \vee \; m > \bar{m} + \Gamma   \\
> & \quad  \bar{m} - \Gamma \leq m \leq \bar{m} + \Gamma \\
\end{cases} \\
\end{gather*}

This allows the bounds for the points of intersection to be concisely written

\begin{equation*}
\begin{array}{ccccc}
 \maximaTheta{b}(m-1) & \lessGtrOp{(m - {\textstyle \frac{1}{2}})}     & \thetaX{2m}       &\lessGtrOp{(m - {\textstyle \frac{1}{2}})}   &   \maximaTheta{a}(m)  \\ \\
 \minimaTheta{b}(m)      & \lessGtrOp{m}     & \thetaX{2m+1}  & \lessGtrOp{m}   &   \minimaTheta{a} (m+1) 
 \end{array}
\end{equation*}

or

\begin{align}
\begin{array}{cc}
  \maximaTheta{b}(m-1) &\lessGtrOp{(m - {\textstyle \frac{1}{2}})}  \\ \\
  \minimaTheta{a} (m)     & < 
\end{array}
\thetaX{2m}
\begin{array}{cc}
  \lessGtrOp{(m - {\textstyle \frac{1}{2}})}  &   \maximaTheta{a}(m)  \\ \\
  <                                                              &   \minimaTheta{b}(m)  
\end{array} \quad
& \quad
\begin{array}{cc}
   \maximaTheta{a}(m)  & <                      \\ \\
   \minimaTheta{b}(m)  & \lessGtrOp{m}    
\end{array}
\thetaX{2m+1}
\begin{array}{cc}
 <                       &  \maximaTheta{b}(m)  \\ \\
\lessGtrOp{m}     &  \minimaTheta{a}(m+1)  
\end{array} 
\end{align}

Adjusting for the intersection index $i$ allows us to see that the intersection points are best classified by being either odd or even values of $i$

\begin{align}
\begin{array}{cc}
  \maximaTheta{b}(\frac{i-2}{2}) &\lessGtrOp{ \frac{i-1}{2} }  \\ \\
  \minimaTheta{a} (\frac{i}{2})     & < 
\end{array}
\thetaX{i} 
\begin{array}{cc}
  \lessGtrOp{ \frac{i-1}{2} }  & \maximaTheta{a}(\frac{i}{2})  \\ \\
  <                                       & \minimaTheta{b}(\frac{i}{2})  
\end{array} \quad
& \quad
\begin{array}{cc}
  \maximaTheta{a}(\frac{i}{2})  & <                       \\ \\
  \minimaTheta{b}(\frac{i}{2})  & \lessGtrOp{ \frac{i}{2} }   
\end{array}
\thetaX{i+1}
\begin{array}{cc}
 <                        &  \maximaTheta{b}(\frac{i}{2})    \\ \\
\lessGtrOp{ \frac{i}{2} }    &  \minimaTheta{a} (\frac{i}{2}+1)  
\end{array} 
\end{align}


\begin{equation*}
\overbrace{
\begin{array}{cc}
  \maximaTheta{b}(\frac{i-2}{2}) &  \lessGtrOp{ \frac{i-1}{2} }  \\ \\
  \minimaTheta{a} (\frac{i}{2})     &  < 
\end{array}
\thetaX{i} 
\begin{array}{cc}
  \lessGtrOp{ \frac{i-1}{2} }  &   \maximaTheta{a}(\frac{i}{2})  \\ \\
  <                                        &   \minimaTheta{b}(\frac{i}{2})  
\end{array} 
}^{i \in \{ 2 \mathbb{N} \} }
\quad
 \quad
 \overbrace{
\begin{array}{cc}
  \maximaTheta{a}(\frac{i-1}{2})  & <                       \\ \\
  \minimaTheta{b}(\frac{i-1}{2})   & \lessGtrOp{ \frac{i-1}{2} }   
\end{array}
\thetaX{i} 
\begin{array}{cc}
 <                                         &    \maximaTheta{b}(\frac{i-1}{2})       \\ \\
\lessGtrOp{ \frac{i-1}{2} }    &     \minimaTheta{a} (\frac{i+1}{2})  
\end{array} 
}^{i \in \{ 2 \mathbb{N} +1 \} }
\end{equation*}
Recalling the definitions of the extrema functions
\begin{align*}
 \minimaTheta{a}\left(\frac{i}{2}\right)     &= \extremaTheta{a}{i}  &  \minimaTheta{b}\left(\frac{i}{2}\right)      &=  \extremaTheta{b}{i}  &  i &\in \{0,2,4,6\cdots2^{n-2} \} \\ 
\maximaTheta{a}\left(\frac{i-1}{2}\right) &= \extremaTheta{a}{i}  &  \maximaTheta{b}\left(\frac{i-1}{2}\right)  &=  \extremaTheta{b}{i}  &  i &\in \{1,3,5,7\cdots2^{n-2}-1\}  
\end{align*}

the boundary relations become

\begin{equation*}
\overbrace{
\begin{array}{cc}
  \extremaTheta{b}{i-1} &  \lessGtrOp{ \frac{i-1}{2} }  \\ \\
  \extremaTheta{a}{i}    &  < 
\end{array}
\thetaX{i} 
\begin{array}{cc}
  \lessGtrOp{ \frac{i-1}{2} }  &   \extremaTheta{a}{i+1}  \\ \\
  <                                        &   \extremaTheta{b}{i} 
\end{array} 
}^{i \in \{ 2 \mathbb{N} \} }
\quad
 \quad
 \overbrace{
\begin{array}{cc}
  \extremaTheta{a}{i}   & <                       \\ \\
  \extremaTheta{b}{i-1}    & \lessGtrOp{ \frac{i-1}{2} }   
\end{array}
\thetaX{i} 
\begin{array}{cc}
 <                                         &    \extremaTheta{b}{i}        \\ \\
\lessGtrOp{ \frac{i-1}{2} }    &     \extremaTheta{a}{i+1}   
\end{array} 
}^{i \in \{ 2 \mathbb{N} +1 \} }
\end{equation*}

\parpic(79mm,104mm)[r]{\fbox{\parbox{76mm}{
\includegraphics[width=76mm]{Chapter2/Figs/GeometryClassifyPoint.jpg}  \\
\centering  $\frac{h_1}{h_2} = \frac{b}{a} \quad h_1 = \frac{b}{a+b} $ \\
\centering $a_{2m}  =     \| \maximaTheta{b}(m-1) -     \maximaTheta{a}(m) \|  $ \\
\centering $b_{2m}  = \minimaTheta{a}(m)     - \minimaTheta{b}(m)$ \\
\centering $a_{2m+1}   =     \maximaTheta{a}(m)     -      \maximaTheta{b}(m)$    \\   
\centering $b_{2m+1}  = \| \minimaTheta{b}(m)     - \minimaTheta{a} (m+1) \| $  \\
}}
}
We want to classify the points of intersection as greater or lesser than a tolerance $0 < \tau \le 1$. In order to do this, we make a linear approximation to the perturbation between the extrema. There is one point of intersection between each of the maxima and corresponding minima. A bit of geometry allows us to write a classification criteria for each of the points of intersection.


Defining a function for the degree of error at the point of intersection using the linear approximation from the four extrema surrounding the point , and generalizing to allow for different maximum errors $\alpha$ for channel $a$ and $\beta$ for channel b. The conditions on $i$ and $\iota$ can be more compactly written by applying a condition to their product.

\begin{equation}\label{eq:generalizedPerturbation}
h(i,\iota, \alpha, \beta)  = \begin{cases}
\frac{\alpha  \beta  \left( \extremaTheta{a} {i+\iota }-\extremaTheta{b} {i-\iota } \right) }
{\beta  \left( \extremaTheta{a} {i+\iota } - \extremaTheta{a} {i+\iota +1}  \right) + \alpha  \left( \extremaTheta{b} {i-\iota -1} - \extremaTheta{b} {i-\iota } \right) }  &
  i + \iota \in \{2 \mathbb{N}     \}  \\
\frac{\alpha  \beta  \left( \extremaTheta{b} {i-\iota -1}-\extremaTheta{a} {i+\iota +1} \right) }
{\beta  \left( \extremaTheta{a} { i+\iota } -\extremaTheta{a} { i+\iota + 1 } \right) +\alpha  \left( \extremaTheta{b} {i-\iota -1} -\extremaTheta{b} {i-\iota } \right) }  & 
i + \iota \in \{2 \mathbb{N}  + 1 \} 
\end{cases}
\end{equation}

allows the criteria for an acceptable perturbation to be written as $\tau \ge h(i,\iota)$. A more useful formulation is to specify an acceptable perturbation to each channel $\tau_\alpha = \frac{\tau}{\alpha}$ and $\tau_\beta = \frac{\tau}{\beta}$. We may as well choose $\tau = 1$  and use the channel specific values to control the acceptable perturbation. 

\begin{gather}\label{eq:generalizedPerturbationCondition}
h(i,\iota, \tau_\alpha, \tau_\beta)  =  \begin{cases}
\frac{ \left( \extremaTheta{a} {i+\iota }-\extremaTheta{b} {i-\iota } \right) }
{\tau_\alpha  \left( \extremaTheta{a} {i+\iota } - \extremaTheta{a} {i+\iota +1}  \right) + \tau_\beta  \left( \extremaTheta{b} {i-\iota -1} - \extremaTheta{b} {i-\iota } \right) }  &
  i + \iota \in \{2 \mathbb{N}     \}  \\
\frac{  \left( \extremaTheta{b} {i-\iota -1}-\extremaTheta{a} {i+\iota +1} \right) }
{\tau_\alpha  \left( \extremaTheta{a} { i+\iota } -\extremaTheta{a} { i+\iota + 1 } \right) +\tau_\beta \left( \extremaTheta{b} {i-\iota -1} -\extremaTheta{b} {i-\iota } \right) }  & 
i + \iota \in \{2 \mathbb{N}  + 1 \} 
\end{cases} \\
1 \ge h(i,\iota, \tau_\alpha, \tau_\beta) 
\end{gather}

Given a desired angle of rotation $\theta$; a data type, which determines $n$; and the acceptable channel perturbations, specified by $\tau_\alpha$ and $\tau_\beta$ we can find the nearest angle which minimises the quantisation error and a close estimate of the maximum error which can occur.

\begin{gather}
 \iota  = \left\lfloor 7\ \meanIndx-\sqrt{i^2 - 2 i \; \meanIndx + 49\ \meanIndx^2 }\right\rfloor  \quad\quad\quad \meanIndx = 2^{n-3}  \\
i = \frac{\meanIndx \tan (\thetaA ) \left(\sqrt{3} \tan (\thetaA )+7\right)}{\tan (\thetaA )+\sqrt{3}} 
\quad \text{where} \quad \thetaA = \theta \bmod \frac{\pi}{6}  
\end{gather}

Starting from the  value $ i $ the algorithm searches for $ \tilde{i} $ the value nearest to $i$ which satisfies the condition $1 \ge h(i,\iota, \tau_\alpha, \tau_\beta)$ (\ref{eq:generalizedPerturbationCondition}).


The algorithm then finds a starting value for the index (\ref{eq:indexFromTheta}) and returns the closest value which satisfies the the condition $1 \ge h(i,\iota, \tau_\alpha, \tau_\beta)$ (\ref{eq:generalizedPerturbationCondition}):

The region $\iota$ for a given value of $i$ is found and the appropriate values (\ref{eq:extremaBounds}) for the extrema (\ref{eq:ExtremaFunctions}) are evaluated in the function which finds the perturbation at the index (\ref{eq:generalizedPerturbation}) and the intercept position $\thetaX{i}$ is found with algorithm \ref{algo:thetaX}.
%\begin{gather*}
%\begin{aligned}
% \iota(i,n)  &= \left\lfloor 7\ \meanIndx-\sqrt{i^2 - 2 i \; \meanIndx + 49\ \meanIndx^2 }\right\rfloor &
%  \extremaTheta{a} {l} = & \arctan\left(\frac{l \sqrt{3} }{2^n - l}\right) &
%  \extremaTheta{b} {l} = & \arctan\left(\frac{l \; 2^{2-n}}{\sqrt{3}}       \right)
%\end{aligned} \\
%\begin{aligned}
% \minimaTheta{a}  &= \extremaTheta{a}{i+\iota} &
% \maximaTheta{b} &= \extremaTheta{b}{i-\iota-1} & 
% \minimaTheta{b}  &= \extremaTheta{b}{i-\iota}  &
% \maximaTheta{a} &= \extremaTheta{a}{i+\iota+1}  &
%  \text{when} \quad 
%i + \iota &\in \{2 \mathbb{N}     \} \\
% \maximaTheta{a}  &= \extremaTheta{a}{i+\iota} & 
% \minimaTheta{b}  &= \extremaTheta{b} {i-\iota-1} &
% \maximaTheta{b} &= \extremaTheta{b}{i-\iota}  &
% \minimaTheta{a}  &= \extremaTheta{a}{i+\iota+1}  & 
%\text{when} \quad 
%i + \iota &\in \{2 \mathbb{N}  + 1 \} 
%\end{aligned}\\
%\thetaX{i} = \left\{\frac{
%\beta \minimaTheta{b} (\minimaTheta{a}  -\maximaTheta{a} )+\alpha  \minimaTheta{a} (\maximaTheta{b}-\minimaTheta{b})  }{
%\beta  (\minimaTheta{a} -\maximaTheta{a})+\alpha  (\maximaTheta{b}-\minimaTheta{b})
%},\frac{
%\alpha  \beta  (\minimaTheta{a}-\minimaTheta{b})  }{
%\beta  (\minimaTheta{a}-\maximaTheta{a})+\alpha  (\maximaTheta{b}-\minimaTheta{b})
%}\right\}
%\end{gather*}

 \begin{algorithm}[h]
 \begin{algorithmic}
      \Function { $\overset{\times}{\phi}$}{$ i, \tau_\alpha, \tau_\beta, n $} \Comment{$i$ integer index of the intercept }
       \State $\meanIndx \gets  2^{n-3}$  ; \quad  $\iota(i,n)  \gets  \left\lfloor 7\ \meanIndx-\sqrt{i^2 - 2 i \meanIndx + 49\ \meanIndx^2 }\right\rfloor $  \Comment{$n$ the source bit depth}
        \State $\extremaTheta{b}{\mathbf{l} } = \arctan\left(\frac{\mathbf{l} 2^{2-n}}{\sqrt{3}}       \right)$ ; \quad 
        $\extremaTheta{a}{\mathbf{l} } = \arctan\left(\frac{\mathbf{l} \sqrt{3} }{2^n - \mathbf{l}}\right)$
        \If{$ i + \iota \bmod{2} = 0$}  \Comment{$i + \iota$ is even}
               \State   $\maximaTheta{a} \gets \extremaTheta{a}{i+\iota+1}  $ ;\quad
                 $\minimaTheta{a}  \gets \extremaTheta{a}{i+\iota} $ 
               \State   $\maximaTheta{b} \gets \extremaTheta{b}{i-\iota-1} $ ;\quad
                 $\minimaTheta{b}  \gets \extremaTheta{b}{i-\iota}  $
         \Else
                \State  $\maximaTheta{a}  \gets \extremaTheta{a}{i+\iota} $ ;\quad
                 $\minimaTheta{a}  \gets \extremaTheta{a}{i+\iota+1}  $ \;
                \State  $\maximaTheta{b} \gets \extremaTheta{b}{i-\iota}  $ ;\quad
                 $\minimaTheta{b}  \gets \extremaTheta{b} {i-\iota-1} $ \;
         \EndIf
       \State  $\thetaX{i} \gets \left\{\frac{
        \tau_\alpha \minimaTheta{b} (\minimaTheta{a}  -\maximaTheta{a} )+\tau_\beta   \minimaTheta{a} (\maximaTheta{b}-\minimaTheta{b})  }{
        \tau_\alpha  (\minimaTheta{a} -\maximaTheta{a})+\tau_\beta   (\maximaTheta{b}-\minimaTheta{b})
        },\frac{
         (\minimaTheta{a}-\minimaTheta{b})  }{
        \tau_\alpha  (\minimaTheta{a}-\maximaTheta{a}) + \tau_\beta   (\maximaTheta{b}-\minimaTheta{b})
        }\right\} $ \;
      \State \textbf{Return} {$\thetaX{i}$}\Comment{$\thetaX{i} $ \{angle,  maximum perturbation\} }
\EndFunction
 \end{algorithmic}
    \caption{A function which returns the angular position of compromise between the perturbations to the channels.}
    \label{algo:thetaX}
\end{algorithm}
%the angular position of the intercept and the maximum perturbation at that value

To find the angular value closest to $\theta$ which produces a perturbation less than the tolerance $\tau$, algorithm \ref{algo:newTheta} finds the closest value which satisfies the tolerance in the positive and negative directions and returns the nearest one to the requested value of $\theta$. Algorithm \ref{algo:newTheta} is guaranteed to find a value which satisfies the tolerance in each direction within a $\frac{\pi}{6}$ region because the perturbation is zero at the ends of the $\frac{\pi}{6}$ region.
 
\begin{algorithm}[H]
 \begin{algorithmic}
  \Require{ $\tau_\alpha$, $\tau_\beta$ the maximum allowed perturbation to the channels}
  \State \phantom{Require}  { $\theta$ the requested angle;}
 \Ensure{ $\vartheta$ the suggested value for $\theta$ and  $h$ the predicted maximum perturbation.}
 \State  \begin{tabular*}{\textwidth}{c @{\extracolsep{\fill}} lll}
 $\delta\theta \gets \theta \bmod{\frac{\pi}{6} }$; & $\Theta \gets \theta - \delta\theta$;  &  $\meanIndx \gets  2^{n-3} $;
 \end{tabular*}
  \State  \begin{tabular*}{\textwidth}{c @{\extracolsep{\fill}} ll}
  $i_{\circleddash} \gets i_{\oplus} \gets \frac{\meanIndx \tan (\delta \theta ) \left(\sqrt{3} \tan (\delta \theta )+7\right)}{\tan (\delta \theta )+\sqrt{3}} $; & 
  $  \left\{ \vartheta_{\circleddash}, h_{\circleddash} \right\} \gets \left\{ \vartheta_{\oplus}, h_{\oplus} \right\} \gets \thetaX{i, \tau_\alpha, \tau_\beta, n}$
  \end{tabular*}
 \While{$h_{\oplus} > 1$}
 \State  $i_{\oplus}++$ \;
 \State  $\left\{ \vartheta_{\oplus}, h_{\oplus}\right\} \gets \thetaX{i_{\oplus}, \tau_\alpha, \tau_\beta, n}$
 \EndWhile
 \While{$h_{\circleddash} > 1$}
  \State   $i_{\circleddash}--$ \;
  \State  $\left\{ \vartheta_{\circleddash}, h_{\circleddash}\right\} \gets \thetaX{i_{\circleddash}, \tau_\alpha, \tau_\beta, n}$
 \EndWhile
  \If{ $\vartheta_{\oplus} -\delta\theta < \delta\theta - \vartheta_{\circleddash}$ }
 \State  $\left\{ \vartheta, h\right\}  = \left\{\Theta + \vartheta_{\oplus}, h_{\oplus}\right\}  $ 
 \Else
  \State $\left\{ \vartheta, h\right\}  = \left\{\Theta + \vartheta_{\circleddash}, h_{\circleddash}\right\} $
  \EndIf
 \State \textbf{Return} {$\left\{ \vartheta, h\right\} $ }
  \end{algorithmic}
    \caption{Suggest a new value for $\theta$}
    \label{algo:newTheta}
 \end{algorithm}
 
 This allows us to construct an integer rotation matrix which is sufficiently close to the floating point rotation matrix that it introduces an error in the rotated pixel values no greater than $\tau_\alpha$ and $ \tau_\beta$ in the two chromatic channels. The only question left is how onerous is the restriction on the choice of angle. The algorithm would be useless if the suggested angle is very different from the requested angle. To investigate this lets look at the special case where no error is allowed.
 The criteria $\tau_\alpha = \tau_\beta = \text{\textonehalf}$ gives an integer rotation matrix which produces the same effect as the floating point rotation matrix. It also selects from either the odd values or the even values of i, because if $h(i,\iota, \tau_\alpha, \tau_\beta)<\text{\textonehalf}$ is true, then $h(i+1,n,, \tau_\alpha, \tau_\beta)<\text{\textonehalf}$  is false and vice versa. There are $2^{n-2}$ points of intersection and $2^{n-3}$ which satisfy the criteria $\tau = \frac{1}{2}$ in each $\frac{\pi}{6}$ region. 

For a 24 bit color image $n$ is 8, so $\intR \in \{ -2^{8-2} \cdots 2^{8-2} \}$, which gives $2^{8-3}-1$ values in each $\frac{\pi}{6}$ region, producing a maximum perturbation of less than \textonehalf. In total, there are $12 (2^{8-3}-1) = 372$ possible values for $\theta$ which produces a maximum perturbation of less than \textonehalf. This would allow the destination pixel values to be expressed in 8-bit numbers without error using a transformation matrix expressed in 6-bit signed integers. The statistics performed to determine $\theta$ are not demanding enough to justify rejecting a suggested value of $\theta$ within 1\textdegree  of the requested value. It is, however, important to know the value actually used in the algorithm, which is why the mechanism for adjusting $\theta$ is separate from the color-space algorithm and is under the control of the programmer.



\chapter{Preservation of Color Information}\label{app:PreservationOfColorInformation}

The goal of the algorithm is to preserve all the information captured by the camera which relates to skin whilst discarding as much of the irrelevant information as possible. Given that edges and features often present as shadows and highlights, all the information captured in terms of luminosity will be regarded as relevant information, at least as far as the manipulation of individual pixel values is concerned. Considering the chromatic information, the importance of the pixel value will be directly determined by a Gaussian distribution.

Knowing the range of values produced by the rotation allows us to scale the transformation to fit into the range of the destination data type. If we have RGB pixel values in a given machine data type, the amount of information contained in each of those channels is equal to the number of values accessible in that data type. For example: for 8-bit, unsigned integers, there are 256 possible values. After a rotation, we are interested in the amount of information which lies along the new axes. This is found simply by multiplying the range of the source data type by the length of the new axes found for the unit cube. To preserve all the information captured, we would therefore have to use a larger data type to store the new values. We are, however, only interested in a small region in the chromatic space. The question is, then, how to preserve the relevant information in a way consistent with the significance indicated by the aforementioned Gaussian distribution.


None of the rotated axes have lengths less than 1 for the unit RGB cube. For this reason we've written re-distribution functions which perform any necessary type conversion whilst preserving the information in a controlled way; we can keep the information where it's needed and discard it where it's irrelevant. So although this is strictly beyond the normal meaning of a color space conversion, it is addressing a connected issue and belongs in the conversion. In terms of optimization, it is also the most efficient place in the code in which to perform this adjustment, allowing us to --- for the sake of example --- discard the details of the colors of a duck's feathers whilst keeping the hues and tones of human skin.

We can use a function to redistribute the information contained on the longer axis onto the shorter axis, which can be expressed in the discrete representation of that axis necessitated by internal integer data types. There are three ways in which to implement the re-distribution functions:

\section{Partition}\label{sec:Partition}

The most straightforward re-distribution method is to simply preserve the information in a 1-to-1 fashion within a region. The region can be defined in terms of the distribution Gaussian by specifying a significance level in terms of the variance or the standard deviation.

\section{Linear}\label{sec:Linear}

A slightly more sophisticated method is to use a linear re-distribution. A linear distribution is equivalent to partitioning given a unit gradient. However, a linear re-distribution function allows for the possibility of data compression. So then, in the case where the region of interest contains more information than can be expressed in the destination data type, a linear distribution function allows an even compression of the information from source to destination. ~\cite{Lee2002}

\section{ERF (Gaussian Error Function)}\label{sec:ERF}

The integral of the cumulative Gaussian (i.e. Error Function) allows the re-distribution of the information on the axis in a way which selectively preserves the information about a point on the axis (i.e. the mean of the Gaussian), and then progressively discard the information as it falls into the tails of the Gaussian. So, it provides a non-linear distribution of the information. The Gaussian can be seen as describing our interest in the information contained along the axis, so it's logical to use the error function to redistribute the information. The disadvantage of this is simply the computational effort involved in generating the error function.

The error function distribution is mathematically correct, being directly related to the Gaussian fit. Computationally, there are two considerations: the numerical representation, and performance. The discrete representation of the numerics means that --- for a significant number of possible distributions --- distributing using the error function has little to no advantage (or indeed difference) from using a linear distribution.

Considering the preservation of information captured, mappings with a gradient greater than 1 are undesirable because they preserve all the information whilst being informatically wasteful in that there are functionally inaccessible discrete values in the destination range. Our stated aim is to preserve the information in the image pertaining to human skin; unevenly distributing this information across a discrete data type is not only wasteful in terms of memory, but also of processing resources because subsequent processing routines will treat the data as if it has a higher fidelity than it actually does.

To construct a distribution function, we first need to describe the relationship of the error function to the Gaussian fit, and then produce a function with the appropriate range and domain. For a Gaussian fit with an amplitude A, a mean of $\mu$, and a standard deviation of $\sigma$, where $\mu$ and $\text{x}$ lie in a source range $\xRange$ from $\xMin$ to $\xMax$. The cumulative distribution is found by integrating from $\xMin$ to the point $\text{x}$, as can be seen in (\ref{eq:ErfDefinition}):

\begin{equation}\label{eq:ErfDefinition}
  \int _{\xMin}^{\text{x}} A \frac{e^{-\frac{(t-\mu )^2}{2 \sigma ^2}}}{\sqrt{2 \pi } \sigma }dt = 
  \frac{1}{2} A \left(\text{erf}\left(\frac{\text{x}-\mu }{\sqrt{2} \sigma }\right)-\text{erf}\left(\frac{\xMin-\mu }{\sqrt{2} \sigma }\right)\right)
\end{equation}

The Gaussian distribution and the cumulative distribution are shown in Figure~\ref{fig:ErrorFunctionGraph} for some values chosen for illustrative purposes. All that is required now is to fix the range $\yMin$ to $\yMax$ for the domain $\xMin$ to $\xMax$.

\begin{figure}[h!] %hi-res
  \caption{Error function.}  \label{fig:ErrorFunctionGraph}
  \centering
    \includegraphics[width=\textwidth]{Chapter2/Figs/errorFunction.jpg}
\end{figure}

First, we determine the maximum value taken in the domain. This is simply found by evaluating the function at $\xMax$. It should be noted that --- if the Gaussian distribution is well contained in the source domain --- the maximum value should be equal to the amplitude. For the sake of simplicity, we'll ignore the amplitude of the fitted Gaussian found previously as it is not relevant to the design of the re-distribution function. So, to fix the range of the distribution function, we first scale to the range $0:1$ by simply dividing through by the maximum value, and then re-scale to the destination range $\yRange = \yMax - \yMin$ and shift by $\yMin$.


\begin{equation}\label{eq:disFunction}
  dis(x) = \frac{(\yRange) \left(\text{erf}\left(\frac{x-\mu }{\sqrt{2} \sigma }\right)-\text{erf}\left(\frac{\xMin-\mu }{\sqrt{2} \sigma }\right)\right)}{\text{erf}\left(\frac{\xMax-\mu }{\sqrt{2} \sigma }\right)-\text{erf}\left(\frac{\xMin-\mu }{\sqrt{2} \sigma }\right)}+\yMin
\end{equation}


\section{Efficiently Implementing the Distribution Function}
Mathematically, the ERF distribution function~(\ref{eq:disFunction}) achieves all the stated objectives. However, on a device we are dealing with discrete numerics and limited processing power, so further analysis is required. Where we're using a discrete domain and range, the distribution is usefully divided into three characteristic behaviours:  where it is constant, where it preserves all the information, and where it selectively preserves information. Looking at the distribution, this divides the source domain into five regions: two where it is effectively constant, two where it is selective, and one region around the mean where it preserves all the information. In order to design an efficient algorithm, it is useful to identify the boundaries of these five regions.


\subsection{The Region Which Discards All Information}
\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{Chapter2/Figs/EffectivlyConstantRange}
\caption{The region which discards all information beyond $\Discard_2$ is shown above. The shaded grid squares show the value actually taken by the discrete distribution.}
\label{fig:EffectivlyConstantRange}
\end{figure}


First, we need to identify where the distribution is effectively constant. This can be found by solving the following equation in the region and domain $0 \le x \le 1$ and then generalized to the specific discrete numerics:

\begin{equation}\label{eq:0to1}
 \frac{\text{erf}\left(\frac{\mu }{\sqrt{2} \sigma }\right)+\text{erf}\left(\frac{x-\mu }{\sqrt{2} \sigma }\right)}{\text{erf}\left(\frac{\mu }{\sqrt{2} \sigma }\right)-\text{erf}\left(\frac{\mu -1}{\sqrt{2} \sigma }\right)}=\text{dL} \quad \text{where} \quad \text{dL} = \left\{ \frac{1}{\yRange}, 1 - \frac{1}{\yRange} \right\}
\end{equation}


The solution is found for the source domain in the range $0:1$ as:


\begin{equation}\label{eq:LowHigh}
 x = \sqrt{2} \sigma  \text{erf}^{-1}\left((\text{dL}-1) \text{erf}\left(\frac{\mu }{\sqrt{2} \sigma }\right)-\text{dL} \; \text{erf}\left(\frac{\mu -1}{\sqrt{2} \sigma }\right)\right)+\mu
\end{equation}


The boundaries of the regions $x < \Discard_1 $ and $ x > \Discard_2$ for $x \in \{\tMin \cdots \tMax \}$ or $x< \discard_1$  and $ x > \discard_2$ for $x \in \{0 \cdots 1 \}$ can be written using the following helpful constants of the distribution.

\begin{align}\label{eq:DistributionConstants}
\Sigma^- &= \text{erf}\left(\frac{\mu -1}{\sqrt{2} \sigma }\right) &
 \Sigma^+ &= \text{erf}\left(\frac{\mu }{\sqrt{2} \sigma }\right) &
  \text{dL} &= \frac{1}{\yRange} &
  \kappa &= \frac{\yRange}{\tRange} 
\end{align}

\begin{equation}\label{eq:DiscardedRegionBounds}
\begin{aligned}
\discard_1 &= \sigma \sqrt{2} \; \text{erf}^{-1}\left((\text{dL}-1) \Sigma^+-\text{dL} \; \Sigma^-\right)+\mu  &            
\discard_2 &= \sigma \sqrt{2} \; \text{erf}^{-1}\left((\text{dL}-1) \Sigma^- -\text{dL} \; \Sigma^+ \right)+\mu \\
\Discard_1&= \tMin + \tRange \left( \discard_1(\mu,\sigma) \right) & 
\Discard_2 &= \tMin + \tRange \left( \discard_2(\mu,\sigma) \right)
\end{aligned}
\end{equation}

\subsection{The Region Which Keeps All Information}
To find the region where all the information in the source domain is preserved, we differentiate the distribution and solve for where the gradient is equal to the destination range over the source range. This corresponds to the point at which a unit change in the source produces a unit change in the destination range:

\begin{equation}\label{eq:Boundaries}
\frac{\sqrt{\frac{2}{\pi }} e^{-\frac{(x-\mu )^2}{2 \sigma ^2}}}{\sigma  \left(\text{erf}\left(\frac{\mu }{\sqrt{2} \sigma }\right)-\text{erf}\left(\frac{\mu -1}{\sqrt{2} \sigma }\right)\right)}=\frac{\yRange}{\tRange}
\end{equation}

Rearranging for $x$, we find:

\begin{equation}\label{eq:PreservedRegion}
 x=\mu \pm \sigma  \sqrt{-2 \log \left(\sigma  \left(\text{erf}\left(\frac{\mu }{\sqrt{2} \sigma }\right)-\text{erf}\left(\frac{\mu -1}{\sqrt{2} \sigma }\right)\right)\right)+2 \log \left(\frac{\tRange}{\yRange}\right)+\log \left(\frac{2}{\pi }\right)}
\end{equation}


The boundaries of the region $\Keep_1 < x < \Keep_2$ for $x \in \{\tMin \cdots \tMax \}$ and $\keep_1 < x < \keep_2$ for $x \in \{0 \cdots 1 \}$ can be written using the following helpful constants of the distribution.

\begin{align}\label{eq:DistributionConstants2}
\Sigma^- &= \text{erf}\left(\frac{\mu -1}{\sqrt{2} \sigma }\right) &
 \Sigma^+ &= \text{erf}\left(\frac{\mu }{\sqrt{2} \sigma }\right) &
  \kappa &= \frac{\yRange}{\tRange} 
\end{align}

\begin{equation}\label{eq:PreservedRegionConsts}
  w(\mu,\sigma)  =  \sigma  \sqrt{ \log \left(\frac{2}{\pi } \right) -2 \log \left(\kappa \sigma  \left(\Sigma^+-\Sigma^-\right)\right) }
\end{equation}

The equations are found in the unit source domain $0:1$. It is a simple matter to scale and shift these values to give the points in a more general source domain.

\begin{equation}\label{eq:PreservedRegionGen}
\begin{aligned}
\keep_1(\mu,\sigma) &=                                    \mu - w(\mu,\sigma) &            \keep_2(\mu,\sigma) &=                                   \mu + w(\mu,\sigma) \\
\Keep_1(\mu,\sigma) &= \tMin + \tRange\left( \mu - w(\mu,\sigma) \right) & \Keep_2(\mu,\sigma) &= \tMin + \tRange \left(\mu + w(\mu,\sigma)\right)
\end{aligned}
\end{equation}

\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{Chapter2/Figs/ExtensionToLinearRegion}
\caption{The region which preserves all information extends beyond the analytic region due to the rounding involved in the discretization. The shaded grid squares show the value actually taken by the discrete distribution. The cord is the tangent to the distribution curve shifted to the next discrete unit below the curve at $\Keep_2$. The point of intersection is the extended boundary at which the distribution begins to discard information.}
\label{fig:ExtensionToLinearRegion}
\end{figure}

One refinement can be made to these values by recognizing that the discrete distribution extends the effectively linear region past the analytic solution by rounding the values. This can be seen in \ref{fig:ExtensionToLinearRegion}, where the shaded squares are the rounded values. The extended region boundary $\eKeep$ was found by numerically solving 

\begin{equation}
\left\lfloor\text{dis}\left(\Keep _2\right)\right\rfloor+\left(x-\Keep _2\right)=\text{dis}(x)
\end{equation}

In the C++ code, whilst numerical routines were used  MatLab and Mathematica to perform the analysis, the extended region boundary point was found by 'walking' along the distribution from the analytic point $\Keep_2$  until divergence from linear behaviour became apparent. This is regarded as a simpler solution, not requiring the use of numerical library routines, and proved to be a quick and elegant solution for the C++ implementation. The extended boundaries $\eKeep_1$ and $\eKeep_2$ are then defined by

\begin{equation}
\begin{aligned}
\left\lceil\text{dis}\left(\Keep _1\right)\right\rceil-\Keep _1 & =\text{dis}\left(\eKeep _1\right)- \eKeep _1 &
\left\lfloor\text{dis}\left(\Keep _2\right)\right\rfloor-\Keep _2 & =\text{dis}\left(\eKeep _2\right)- \eKeep _2 
\end{aligned}
\end{equation}

\subsection{The Compression Ratio}

There is one final value of interest to the development of the algorithm, which is the gradient at the mean. The reason this is of interest is because we're trying to compress the relevant data as much as possible. If the destination region is small (i.e. the destination machine type is smaller than the source type), then the gradient at the mean allows us to assess the fidelity required of the source type. If it weren't for the fact that the source is the result of a rotation transformation, then there would be little purpose in assessing this value. However, it is entirely possible that the lengthening of the axes resulting from the rotation is insignificant for the desired destination type; there's no point preserving information during the rotation which is then discarded by the re-distribution. The gradient is given by

\begin{equation}\label{eq:gradient}
\begin{aligned}
\Delta(\mu,\sigma) &= \kappa  \delta(\mu,\sigma)  & \delta(\mu,\sigma)  &= \frac{ \sqrt{2} }{ \sigma \sqrt{\pi }  \left(\Sigma^+-\Sigma^-\right)}
\end{aligned}
\end{equation}

The compression ratio is at most one-to-one, therefore $\kappa <=1$ and the gradient in the unit space must always be greater than one $1 \le \delta(\mu,\sigma)$. so $ \kappa \le \Delta(\mu,\sigma) \le \delta(\mu,\sigma)$.

The required fidelity in the source domain can be found using $ \Delta$ in the sense that the correspondence between one information step in the source must produce a step of $\Delta$ in the destination type. For the algorithm evaluating the maximum gradient $\Delta$ allows us to be sure that  $\Keep$ --- the region on the x axis where all the information is to be preserved --- exists if $\Delta >1$ or tells us that the x axis can be shortened if  $\Delta < 1$. In the algorithm $\Delta$ is used to find an appropriate working data type for the rotated color space and to define the axis scaling for the rotation matrix.


We need to consider the requested compression of information alongside the spread of information caused by the rotation, and the desired focus on the specific region of interest dictated by the statistics. Each axis in the color space is to be represented by a discrete set of numbers. The size of these sets dictates the discretization of the axis, and the ratios between them indicates the spread or compression of the information they contain. We assume that the RGB axes are each discretized to the same extent, each containing $\srcRange$ values. After the application of the un-normalized rotational transformation, the axes contain differing numbers of values given by $\tRange_1 =\srcRange \; \sqrt{3} $, $\tRange_2 = \srcRange \; L_2(\theta) $  and $\tRange_3 = \srcRange \; L_3(\theta)$. These axis lengths preserve all the information contained in the source color space, and so are the maximum length the axis should take. The minumum length the axis may take is where the information is lost evenly throuought the axis, and corresponds to an axis length equal to the destination axis length $\tRange = \dstRange$. 

We can now write an algorithm which determines the necessary scaling for the axes, and whether truncation of the extreme values is significant.
As this will alter $\tRange$ we fix the values of $\kappa$ and $\Delta$ to be those for the working range $\tRange =  \inS[\theta]\srcRange$  which preserves all information. With this value the constants are
\begin{align}
K & =  \frac{\dstRange}{\srcRange}  &
\kappa(\theta) &=  \frac{K}{\mathbf{L}(\theta)}  & 
\Delta(\mu,\sigma)  &= \frac{K}{\mathbf{L}(\theta)} \delta(\mu,\sigma)  
\end{align}
The length of the axis $\tRange$ after rescaling should be

\begin{equation}\label{eq:CombinedRotationRange}
\begin{aligned}
 \tRange(\theta,\mu,\sigma) &= \min\left\{ \frac{K}{\mathbf{L}(\theta)}   \delta(\mu,\sigma) , 1 \right\}  \inS[\theta]\srcRange  \\
  & =  \min\left\{K \delta(\mu,\sigma) \right. ,  \left.  \inS[\theta]\right\}  \srcRange
\end{aligned}
\end{equation}

The scaling $\srcRange$ comes from the source pixel values, the remaining terms translates simply into a rotation matrix scaling

\begin{equation*}
\R[\theta]   =  \min\left\{K \delta(\mu,\sigma) \right. ,  \left.  \inS[\theta]\right\} \otimes \Scale[\theta] \otimes \fRO \left[   \qRe[ \theta, n ] \right]   \\
\end{equation*}

This satisfies the requirements placed on the gradient $\Delta(\mu,\sigma)>1$ because: if we substitute for $\tRange$ in the definition for the gradient \ref{eq:gradient}

\begin{equation}\label{eq:newGradient}
\begin{aligned}
\Delta(\mu,\sigma) % &= \frac{\dstRange}{\tRange}  \delta(\mu,\sigma)  & \delta(\mu,\sigma)  &= \frac{ \sqrt{2} }{ \sigma \sqrt{\pi }  \left(\Sigma^+-\Sigma^-\right)} \\
% &= \frac{\dstRange}{\min\left\{ \frac{K}{\mathbf{L}(\theta)}   \delta(\mu,\sigma) , 1 \right\}  \inS[\theta]\srcRange }  \delta(\mu,\sigma) \\
%&= \frac{1}{\min\left\{ \frac{K}{\mathbf{L}(\theta)}   \delta(\mu,\sigma) , 1 \right\} }  \frac{K}{\mathbf{L}(\theta)}   \delta(\mu,\sigma) \\
%&= \begin{cases}
%1 & \frac{K}{\mathbf{L}(\theta)}   \delta(\mu,\sigma) < 1 \\
% \frac{K}{\mathbf{L}(\theta)}   \delta(\mu,\sigma)  & \frac{K}{\mathbf{L}(\theta)}   \delta(\mu,\sigma) > 1 \\
%\end{cases} \\
&= \max\left\{ \frac{K}{\mathbf{L}(\theta)}   \delta(\mu,\sigma) , 1 \right\}
\end{aligned}
\end{equation}
And the compression ratio $\kappa$ is also explicitly restricted to being at most one and now is no longer defined in terms of $\tRange$.
\begin{equation}\label{eq:newCompressionRatio}
\begin{aligned}
%\kappa(\theta)  &=  \frac{\dstRange}{\tRange} \\
% &= \frac{\dstRange}{\min\left\{ \frac{K}{\mathbf{L}(\theta)}   \delta(\mu,\sigma) , 1 \right\}  \inS[\theta]\srcRange }   \\
%&= \frac{1}{\min\left\{ \frac{K}{\mathbf{L}(\theta)}   \delta(\mu,\sigma) , 1 \right\} }  \frac{K}{\mathbf{L}(\theta)}    \\
%&= \begin{cases}
%\frac{1}{\delta(\mu,\sigma)}   & \frac{K}{\mathbf{L}(\theta)}   \delta(\mu,\sigma) < 1 \\
% \frac{K}{\mathbf{L}(\theta)}   & \frac{K}{\mathbf{L}(\theta)}   \delta(\mu,\sigma) > 1 \\
%\end{cases} \\
%&= \max\left\{ \frac{\mathbf{L}(\theta)}{K \delta(\mu,\sigma) }   , 1 \right\} \frac{K}{\mathbf{L}(\theta)}  \\
\kappa(\theta) &= \max\left\{ \frac{1}{\delta(\mu,\sigma) }   , \frac{K}{\mathbf{L}(\theta)} \right\}   
\end{aligned}
\end{equation}



%
%\begin{equation}
%\R[\theta]  = 
%\begin{cases}
%  \inS[\theta]\otimes \Scale[\theta] \otimes \fRO \left[  \qRe[ \theta, n ] \right]    & \Delta(\mu,\sigma) = 1 \\
%                                        K  \Scale[\theta] \otimes \fRO \left[  \qRe[ \theta, n ] \right]    & \Delta(\mu,\sigma)  = \kappa \\
%        K  \delta(\mu,\sigma)  \Scale[\theta] \otimes \fRO \left[ \qRe[ \theta, n ] \right]    &  \kappa < \Delta(\mu,\sigma) < 1 \\
%\end{cases}
%\end{equation}
%
%\begin{equation}\label{eq:RescaleAxis}
%\R[\theta]  = \min\left\{\Delta(\mu,\sigma), 1 \right\}  \inS[\theta]\otimes \nS[\theta] \otimes \rS \otimes \fSe (\theta) \otimes\qS[ n ] \otimes \fSs[\theta]  \otimes \fRO \left[ \qRe[ \theta, n ]  \right]  
%\end{equation}
%
%
%\begin{equation}
%\R[\theta]  = 
%\begin{cases}
%   \rS \otimes \fSe (\theta) \otimes\qS[ n ] \otimes \fSs[\theta]  \otimes \fRO \left[ \qRe[ \theta, n ]  \right]   & \Delta(\mu,\sigma) = 1 \\
% \kappa \otimes  \inS[\theta] \otimes \nS[\theta] \otimes \rS \otimes \fSe (\theta) \otimes\qS[ n ] \otimes \fSs[\theta]  \otimes \fRO \left[ \qRe[ \theta, n ]  \right]   & \Delta(\mu,\sigma)  = \kappa \\
% \Delta(\mu,\sigma)  \inS[\theta] \otimes \nS[\theta] \otimes \rS \otimes \fSe (\theta) \otimes\qS[ n ] \otimes \fSs[\theta]  \otimes \fRO \left[ \qRe[ \theta, n ]  \right]    &  \kappa < \Delta(\mu,\sigma) < 1 \\
%\end{cases}
%\end{equation}
%
%There are several simplifications which can be applied here. Firstly remembering that :
%\begin{align}
%\srcRange &= 2^n &
%\nS[\theta] \otimes \;  \inS[\theta] & = 1 \\
%\nS[\theta] \otimes \rS \otimes \fSe (\theta) & = 
%\begin{pmatrix}
% \frac{1}{3}  \\
% \frac{1}{2}\\
% \frac{1}{2} \\
%\end{pmatrix} &
%\qS[ n ] & = 
%\begin{pmatrix}
% 1  \\
% 2^{2-n } \\
% 2^{2-n }  \\
%\end{pmatrix} \\
%\rS \otimes \fSe (\theta) & = 
%\begin{pmatrix}
% \frac{1}{3}  \\
% \frac{1}{2}\\
% \frac{1}{2} \\
%\end{pmatrix}  \otimes \;  \inS[\theta]&
%\srcRange \; \qS[ n ] & = 
%\begin{pmatrix}
% 2^n  \\
% 4 \\
% 4  \\
%\end{pmatrix} 
%\end{align}
%
%
%\begin{equation}
%\R[\theta]  = 
%\begin{cases}
%\rS \otimes \fSe (\theta) \otimes\qS[ n ] \otimes \fSs[\theta]  \otimes \fRO \left[ \qRe[ \theta, n ]  \right]  & \Delta(\mu,\sigma) = 1 \\
%\nS[\theta] \otimes \rS \otimes \fSe (\theta) \otimes\qS[ n ] \otimes \fSs[\theta]  \otimes \fRO \left[ \qRe[ \theta, n ]  \right]  & \Delta(\mu,\sigma)  = \kappa \\
%\kappa(\theta) \otimes \rS \otimes \fSe (\theta) \otimes\qS[ n ] \otimes \fSs[\theta]  \otimes \fRO \left[ \qRe[ \theta, n ]  \right]  &  \kappa < \Delta(\mu,\sigma) < 1 \\
%\end{cases}
%\end{equation}
%
%\begin{align}\label{eq:RescaleAxis}
%\R[\theta] % & =  \min\left\{ \Delta(\mu,\sigma) , 1\right\} \otimes \srcRange \otimes \;  \inS[\theta]\otimes \nS[\theta] \otimes \rS \otimes \fSe (\theta) \otimes\qS[ n ] \otimes \fSs[\theta]  \otimes \fRO \left[ \qRe[ \theta, n ]  \right]  \\
%& = \min\left\{ \Delta(\mu,\sigma) , 1\right\}  \otimes \rS \otimes \fSe (\theta) \otimes
%\begin{pmatrix}
%2^n  \\
% 4 \\
%4  \\
%\end{pmatrix}\otimes \fSs[\theta]  \otimes \fRO \left[ \qRe[ \theta, n ]  \right]  \\
%& = \min\left\{ \Delta(\mu,\sigma) , 1\right\}  \otimes \;  \inS[\theta]\begin{pmatrix}
% \frac{1}{3}  \\
% \frac{1}{2}\\
% \frac{1}{2} \\
%\end{pmatrix} \otimes
%\begin{pmatrix}
%2^n  \\
% 4 \\
%4  \\
%\end{pmatrix}\otimes \fSs[\theta]  \otimes \fRO \left[ \qRe[ \theta, n ]  \right]  \\
%& = \min\left\{ \Delta(\mu,\sigma) , 1\right\}  \otimes \;  \inS[\theta]\otimes \begin{pmatrix}
% \frac{2^n}{3}  \\
% 2\\
% 2 \\
%\end{pmatrix} \otimes \fSs[\theta]  \otimes \fRO \left[ \qRe[ \theta, n ]  \right]  \\
%\end{align}
%
%We can now re-formulate the transformation matrix in three different ways: pure rotation without rescaling, 
%
%\begin{equation}
%\R[\theta]  = 
%\begin{cases}
%\rS \otimes \fSe (\theta) \otimes\qS[ n ] \otimes \fSs[\theta]  \otimes \fRO \left[ \qRe[ \theta, n ]  \right]  & \Delta(\mu,\sigma) \ge 1\\
%\nS[\theta] \otimes \rS \otimes \fSe (\theta) \otimes\qS[ n ] \otimes \fSs[\theta]  \otimes \fRO \left[ \qRe[ \theta, n ]  \right]  & \begin{array}{c} \delta(\mu,\sigma) = 1 \\
%  \Delta(\mu,\sigma)  = \kappa
% \end{array} \\
%\kappa[\theta] \otimes \rS \otimes \fSe (\theta) \otimes\qS[ n ] \otimes \fSs[\theta]  \otimes \fRO \left[ \qRe[ \theta, n ]  \right]  &  \kappa < \Delta(\mu,\sigma) < 1 \\
%\end{cases}
%\end{equation}
%\begin{align*}
%\nR[\theta] & = \nRa[\theta]+\dnR[\theta] \\
%& = \nS[\theta] \otimes \rS \otimes \fSe (\theta) \otimes\qS[ n ] \otimes \fSs[\theta]  \otimes \fRO \left[ \qRe[ \theta, n ]  + \dqRe[ \theta, n ]  \right]  \\
%& = 
%\begin{pmatrix}
% \frac{1}{3}  \\
% \frac{1}{2}\\
% \frac{1}{2} \\
%\end{pmatrix}
% \otimes
%\begin{pmatrix}
% 1  \\
% 2^{2-n } \\
% 2^{2-n }  \\
%\end{pmatrix}
%\otimes  \fSs(\theta) \otimes \fRO \left[ \qRe[ \theta, n ]  + \dqRe[ \theta, n ]  \right]   \\
%\end{align*}
%
%\begin{equation}\label{eq:Rotation}
% \R_{xyz}(\theta) =
% \begin{pmatrix}
%  \frac{1}{\sqrt{3}} & \frac{1}{\sqrt{3}} & \frac{1}{\sqrt{3}} \\
%  -\sqrt{\frac{2}{3}} \sine{\theta +\Pii{6} } & \sqrt{\frac{2}{3}} \cos (\theta ) & -\sqrt{\frac{2}{3}} \sine{\Pii{6}-\theta } \\
%  -\sqrt{\frac{2}{3}} \cos \left(\theta +\Pii{6}\right) & -\sqrt{\frac{2}{3}} \sin (\theta ) & \sqrt{\frac{2}{3}} \cos \left(\Pii{6}-\theta \right) \\
% \end{pmatrix} \quad \text{for} \quad  
% \begin{array}{c} \Delta(\mu,\sigma) \ge 1 
% \end{array}
%\end{equation}
%
%maximum scaling --- which scales to the destination range
%
%
%\begin{equation}\label{eq:NormRxyz2}
% \overline{\R}_{xyz}(\theta) =
%\kappa
%\bigotimes
%\R_{xyz}(\theta) \quad where \quad
%\begin{array}{c}
%\kappa = \frac{\dstRange }{\srcRange \mathbf{L}(\theta)}
%\end{array} \quad \text{for} \quad    
% \begin{array}{c} \delta(\mu,\sigma) = 1 \\
%  \Delta(\mu,\sigma)  = \kappa
% \end{array}
%\end{equation}
%
% and scaled --- which shortens the axis as much as possible without losing statistically relevant data. 
%
%\begin{equation}\label{eq:CompressedRotation}
% \widetilde{\R}_{xyz}(\theta,\mathbf{\mu},\mathbf{\sigma}) =
%\Delta(\mu,\sigma)
%\bigotimes
%\R_{xyz}(\theta) \quad \text{for} \quad  \kappa < \Delta(\mu,\sigma) < 1
%\end{equation}
%because $\Delta$ is always greater or equal to $\kappa$ these special cases can be combined into a general scaling for each axis
%
%\begin{equation}\label{eq:CombinedRotation}
% \widehat{\R}_{xyz}(\theta,\mu,\sigma) =
%\left(
%\begin{array}{c}
%\min\left\{\Delta(\mu_1,\sigma_1), 1 \right\} \\
%\min\left\{\Delta(\mu_2,\sigma_2), 1 \right\}  \\
%\min\left\{\Delta(\mu_3,\sigma_3), 1 \right\}  \\
%\end{array}
%\right)
%\bigotimes
%\R_{xyz}(\theta) \quad where \quad
%\begin{array}{c}
%\kappa = \frac{\dstRange }{\srcRange \mathbf{L}(\theta)} \\
%\Delta(\mu,\sigma) = \kappa \delta(\mu,\sigma)
%\end{array}
%\end{equation}

\subsection{A Piecewise Approximation to the ERF Distribution}
We now have equations which give us the four points in the source domain which mark the boundaries of the five characteristic regions. We can now use them to define a piecewise function which uses the computationally problematic error function based distribution as little as possible

\begin{equation}
pDis(x) = \begin{cases}
\dstMin & x \le \Discard_1 \\
\text{dis}(x) & \Discard_1 < x < \eKeep_1 \\
x - \eKeep_1 + \text{dis}(\eKeep_1) & \eKeep_1 \le x \le \eKeep_2  \\
\text{dis}(x) + \eKeep_2 - \eKeep_1 - \text{dis}(\eKeep_2) + \text{dis}(\eKeep_1)  & \eKeep_2 < x < \Discard_2  \\
\text{dis}(\Discard_2) + \eKeep_2 - \eKeep_1 - \text{dis}(\eKeep_2) + \text{dis}(\eKeep_1) & x \ge \Discard_2 \\
\end{cases}
\end{equation}

All three distribution techniques described earlier ( \ref{sec:Partition} , \ref{sec:Linear} , \ref{sec:ERF} ) are special cases of this distribution. When the distribution has a very large variance the piecewise distribution can be simplified as a linear distribution Fig \ref{fig:useLinear}. When the distribution has a very small variance a partitioning is more appropriate Fig \ref{fig:usePartitioning}. The most interesting distributions, however, are the ones which require the use of the piecewise distribution Fig \ref{fig:usePiecewiseERF}. 

\begin{figure}[h]
\centering
\includegraphics[width=1\linewidth]{Chapter2/Figs/useLinear}
\caption{Here the code will decide to use a linear re-distribution.}
\label{fig:useLinear}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=1\linewidth]{Chapter2/Figs/usePartitioning}
\caption{With these values the code will likely decide to use partitioning if the tolerance $\tau_{\text{partitioning}}$is greater than $\Discard_2-\eKeep_2$.}
\label{fig:usePartitioning}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=1\linewidth]{Chapter2/Figs/usePiecewiseERF}
\caption{With these values the code will decide to use a piecewise ERF based distribution. (Labeled "Compact Distribution" in the figure.)}
\label{fig:usePiecewiseERF}
\end{figure}

