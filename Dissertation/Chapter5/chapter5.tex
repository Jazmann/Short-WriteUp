%*****************************************************************************************
%*********************************** Fourth Chapter **************************************
%*****************************************************************************************

\chapter{Future Work and Discussion}

\ifpdf
    \graphicspath{{Chapter5/Figs/Raster/}{Chapter5/Figs/PDF/}{Chapter5/Figs/}}
\else
    \graphicspath{{Chapter5/Figs/Vector/}{Chapter5/Figs/}}
\fi
\section{Results and Evaluation}\label{sec:ResultsAndEvaluation}

The primary focus of this project is to develop and implement a color-space algorithm that is efficient enough to run on mobile devices, but also accurate enough for broad applications. In Chapter 2, a color-space conversion method was developed which simultaneously avoided using floating-point operations and preserved all the chromatic information in the source color-space. Preserving all the chromatic information from the source color-space is something which is not even achieved by floating-point methods because --- even if we assume floating point numbers are represented with infinite precision --- these methods only use the floating point representation internally and lose information when converting back to the integer data types for the rotated chromatic channels (see Appendix \ref{app:PreservationOfColorInformation}). 



\subsection{Timing the Methods}\label{sec:TimingTheMethods}

To evaluate the integer-based color-space conversion method in comparison with an equivalent floating point method, a floating point color-space conversion was implemented which first converted the pixel value into a double precision unit range, performed the rotation using a double precision rotation matrix and then redistributed with the redistribution function described in Chapter 2 to the destination integer range. This is compared below with the optimized integer method presented in Chapter 2.

Obtaining clean running times for multi-core CPUs running multiple threads is problematic. To account for the variability in performance, the two algorithms were run using the same input 1,000 times. Theoretically, the time should be the same for every run with the same input, however, as can be seen in Figure \ref{fig:TimingVariationPlot}, the timings vary away from a baseline with a statistical positive error. It is reasonable to assume that this variation is due to interrupts from system processes, and so the algorithm performance is best measured by taking the minimum runtime from this set of 1,000 runs. 

For the image used in this test, it can be seen that the integer method is about 4.9 times faster than the floating point method. This is a significant gain given that the color-space transform is the most computationally-intensive operation in the entire algorithm.

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.95\textwidth]{Chapter5/Figs/Timing_Variation_Plot.jpg}
    \caption{Timing variation plot. Comparison of process run times using the floating point method and the integer optimized method with the same image input data.}\label{fig:TimingVariationPlot}
\end{figure}

\subsection{Optimization Variation with Input}\label{sec:OptimizationVariationWithInput}

Now we evaluate the performance of the algorithm with differing input images. For the purpose of this test, images generated with varying tonal characteristics is done by generating images with random pixel values within random ranges. Each image was processed 1,000 times and the minimum runtime from those 1,000 runs was taken to be an indication of the difficulty of that image when presented to the two algorithms. The algorithms were run over 100 different images, and the mean and the standard deviation from that mean were found for each of the two methods. The performance of the algorithms can be seen in Figure \ref{fig:ToneVariationPlot} below.

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.95\textwidth]{Chapter5/Figs/Tone_Variation_Plot.jpg}
    \caption{Tone variation plot.}\label{fig:ToneVariationPlot}
\end{figure}

Here the performance gain can be seen to be slightly lower than for the test image used above. However, it can be confidently said that the method presented in Chapter 2 is 4-to-5 times faster than using the usual floating point methods.

\section{Future Applications}\label{sec:FutureApplications}


\subsection{Medical Applications}\label{sec:MedicalApplications}

The benefit of using the color-space conversion algorithm in medical applications is that it's lossless without negatively affecting performance. This is especially beneficial when performing comparisons between chromophores with very subtle differences such as oxyhemoglobin and deoxyhemoglobin or the two types of eumelanin, as losing information makes telling them apart that much harder. These two benefits of the algorithm immediately suggest a number of possible future applications.

Possible applications of the algorithm are: monitoring inflammation, mole mapping and monitoring (Figure \ref{fig:MelanomaImages}), checking eye health and detecting anaemia.

As for applications beyond phone cameras, a number of different medical cameras have been designed for medical diagnostics and telemedicine (see Figure \ref{fig:MedicalExaminationCameras}). However, these cameras also tend to use the RGB color-space, which again has the disadvantage of requiring the examiner to perform a naked-eye comparison, relying on their knowledge and experience to make a diagnosis. By adapting our color-space to target specific chromophores, it may be possible to accurately diagnose any number of conditions without relying as much on expertise. 


%One possible application of the algorithm is monitoring inflammation. By adapting the color-space to distinguish between healthy and inflamed tissue, one might be able measure the degree of inflammation over a period of time by, for the sake of example, photographing parts of the user's body prone to inflammation throughout the day and using the algorithm to perform the comparison. This has the benefit of measuring the degree of inflammation without having to rely on subjective reporting, and could allow individuals to discover triggers for conditions like eczema, arthritis and other such inflammatory diseases. Combining this approach with diet and lifestyle monitoring functions --- other factors which may contribute to inflammation --- could serve as a simple and empowering means of increasing users' quality of life.
%
%In mole mapping, a series of photographs of moles are taken over a period of time to track changes in size, color or shape in order to detect conditions such as malignant melanoma. However, identifying changes in such skin lesions is often limited to the doctor performing naked-eye comparisons between RGB photos of the areas of concern (Figure \ref{fig:MelanomaImages}). This poses a problem when checking for changes in color, in part due to the RGB color-space not separating the luminosity from the chromatic information and differences in lighting conditions. Using our color-space, it is possible to highlight such changes more clearly; the color-space can be adapted to detect changes in coloration between successive skin lesion images, thereby eliminating the need for guesswork. Additionally, by floating the mean the algorithm can account for variable lighting conditions.
%
%Blood flow patterns in the retina are a good indication of eye health. As such, a retina imaging app for monitoring these blood flow patterns is another potential application of the algorithm. It is possible to adapt the color-space to discriminate between oxygenated and de-oxygenated blood flow and use the image alignment such that, with a simple lens manipulator which fits over the phone's camera, one could photograph their own retina daily and measure the difference overtime. The alignment algorithms could be used to monitor any burst blood vessels, even within the eyeball itself. Another, similar application would involve detecting for anaemia; the skin under the eyelids lacks pigmentation, appearing pale if there is little blood. One might be able to take a photo of the skin under the eyelids with their phone camera and use the algorithm to check for a lack of blood flow.
%
%As for applications beyond phone cameras, a number of different medical cameras have been designed for medical diagnostics (see Figure \ref{fig:MedicalExaminationCameras}), especially for use in telemedicine and thermal imaging, many of which are commercially available. These cameras are typically HD video cameras, some of which support a variety of attachments for different types of examinations such as tongue depressors for laryngoscopy. However, these cameras also tend to use the RGB color-space, which again has the disadvantage of requiring the examiner to perform a naked-eye comparison, relying on their knowledge and experience to make a diagnosis. By adapting our color-space to target specific chromophores, it may be possible to accurately diagnose any number of conditions without relying as much on expertise. 

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.45\textwidth]{Chapter5/Figs/DermLite-DL1.jpg}
    \caption{DermLite DL1; A dermatoscope attachment available for several smart devices, used in diagnosing skin disorders.}\label{fig:DermatoscopePhone}
\end{figure}

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.45\textwidth]{Chapter5/Figs/melanoma-images.jpg}
    \caption{A skin lesion photographed under different lighting conditions. (Photos by Kevin Jakob, Illingworth Research.)
    Identifying changes in such skin lesions is often limited to the doctor performing naked-eye comparisons between RGB photos of the areas of concern . This poses a problem when checking for changes in color, in part due to the RGB color-space not separating the luminosity from the chromatic information and differences in lighting conditions. Using our color-space, it is possible to highlight such changes more clearly; the color-space can be adapted to detect changes in coloration between successive skin lesion images, thereby eliminating the need for guesswork. Additionally, by 'floating the mean' the algorithm can account for variable lighting conditions.}\label{fig:MelanomaImages}
\end{figure}

\begin{figure}[h!]
  \centering
    \includegraphics[width=0.45\textwidth]{Chapter5/Figs/TEHD_1.png}
    \includegraphics[width=0.45\textwidth]{Chapter5/Figs/examination-clinical-dermoscan-x2.jpg}
    \caption{A few different medical examination cameras. }\label{fig:MedicalExaminationCameras}
\end{figure}
%
%\subsection{Scientific Applications}\label{sec:ScientificApplications}
%
%\subsubsection{A Study of Human Chromatic Diversity}\label{sec:AStudyOfHumanChromaticDiversity}
%In Chapter 3, we used a sample set of skin images from three individuals; it was interesting to see, in the lumiochromatic color-space LCaCb, how close these skin statistics were to each other in the chromatic plane. It would also be interesting to perform the work of Chapter 3 for a large and chromatically diverse group of people. An attempt was made to look at a diverse sample of individuals using images collected in the Humanae project (\cite{Dass2012}). This failed, however, as we had no control over the camera or the image compression performed before making them available on the Internet. Ultimately, it would be interesting to see if it were possible to estimate the levels of the skin chromophores identified in Chapter 1 using the skin statistics of the individual.
%
%\subsubsection{Chromatic Statistics for Injuries and Defects}\label{sec:ChromaticStatisticsForInjuriesAndDefects}
%If we have the normal skin statistics for an individual, we could take images of a site of injury and build the chromatic histogram (Figure \ref{fig:Fill_the_Bins}). We could find the difference between the sets and create a profile for the injury or defect. In order to do this, it would be useful to have built the statistics for the injury site prior to injury; this is an issue if this is to be done in-lab as it would necessitate causing the injury or defect in order to study the effect. This could be overcome using crowdsourcing. For instance, one might go skydiving as a hobby and so would often bruise their legs, so we could build the skin statistics for their uninjured skin and simply wait for a bruise to appear. 
%
%An issue of this approach would be the consistency of lighting conditions; this could be overcome if we have adapted the method to allow us to set a new white point as outlined in Section \ref{sec:GeneralizingTheRotation}. We could take an uninjured skin patch for which we already have a previous image from which we build the skin statistics, and comparing the two would allow us to find a white point which accounts for the ambient lighting conditions. Our generalized transform could then compensate for the changing lighting conditions. Any difference in the statistics now is due to the injury, and our theoretical model built in Chapter 1 should allow us to interpret this in terms of the chromophores, the key one being the bilirubin, which is a chromophore which causes the yellow-and-green coloration in bruises.
%
%\subsubsection{Repetitive Strain Study}\label{sec:RepetitiveStrainStudy}
%The algorithm we developed in Chapter 4 focuses on the mechanical stress to the fingertip when pressing on a surface; in the Figures \ref{fig:ICWaSResultJSkin}, \ref{fig:ICWaSResultFSkin} and \ref{fig:ICWaSResultNSkin}, it can be seen that the stress also shows in the knuckle. In fact, the mechanical strain can be seen in all the joints of the hand. This suggests that we could build an algorithm which monitors the stress in the joints of the hand by observing the blood flow using the routines developed above. By monitoring the strain put on the joints of an individual's hands while they perform a task, we could measure the overall strain that they have placed on their finger joints. If we combine this with the techniques to measure inflammation presented in Section \ref{sec:MedicalApplications}, we could get a measure of the damage suffered by the individual. 
%
%Once developed, such a monitoring system could be used in studies which assess different ergonomic designs and workplace practices. 
%
%\subsection{Computer Vision Applications}\label{sec:ComputerVisionApplications}
%
%\subsubsection{A Novel Canny Edge Detection Algorithm}\label{sec:ANovelCannyEdgeDetectionAlgorithm}
%The trinary classification developed in Chapter 4 can also be used for an edge detection routine which operates similarly to the Canny edge detection. The Canny edge detection operates by finding edges using a high threshold which is then lowered as the routine traces along the edge, allowing the edge to be extended from a strong edge into a weak edge. This also allows Canny edge detection to "hurdle" over minor defects in the image of the edge. The trinary classified image can be binarized and presented to standard edge detection techniques, thus we can find all of the strong edges. 
%
%The edges which end surrounded by possible "probably not skin" values can easily be found and marked. Other marked ends which lie close to each other are joined if the pixel values on the path between them are all "probably not skin" (i.e. "1") or higher. This is similar to the Canny edge detection in that it allows us to join edges by using a lower threshold across the joint. It differs from Canny, however, in that it does not allow the edge to extend into a lower threshold edge. This would be undesirable using the trinary classification because the low threshold "probably not skin" classification often appears in patches which extend across the true edge often resulting from shadow or reflection. This method could be refined by taking into account the orientation of the edges at their end points, which could be used to determine how edges should actually be connected. This is similar to the "hurdle" method presented in Section \ref{sec:HurdleMethod}, but applied to edges.
%
%\subsubsection{Finger Feature Detection}\label{sec:FingerFeatureDetection}
%The modeling techniques developed in Chapter 4 can be relatively straightforwardly extended to model the entire digit, or even the whole hand. In Chapter 4, we found that the distal width could be used to find a good estimate for the position of the knuckle using the distal, middle and proximal ratios. With this initial model, the next question is what features can we expect to find at these positions in the image? The luminosity axis shows a striking feature at the knuckle position: the knuckle wrinkles. These wrinkles are consistent to each knuckle (i.e. when flexed, they present in the same way), and they're easily identified by orientation, running across the digit as opposed to along it. Additionally, the knuckle wrinkles curve around the center of the knuckle. This could be used to refine the model and track the knuckles. In the chromatic channels, they also display blood flow when flexed.

\section{Color Space Algorithm Improvements}\label{sec:ColorSpaceAlgorithmImprovements}
Here we suggest several improvements which could be made to the color-space algorithm developed in Chapter 2. The improvements presented herein address generalizing the algorithm to accommodate the characteristics of cameras other than the iPhone's. We consider three adaptations: one to allow for multi-spectral cameras, another to allow for cameras which do not have a fixed white point, and a third to handle RAW images captured from the CCD without the iPhone's pre-processing. In practice, these three adaptations need to be combined. Hopefully it shouldn't be too difficult to imagine how this could be done, but in terms of practically proceeding, each one would need to be tackled individually first.

\subsection{Generalizing the Rotation}\label{sec:GeneralizingTheRotation}
In Chapter 2, we derived an expression for a rotation matrix consisting entirely of integers which rotates the RGB pixel values into a custom color-space with a luminosity axis and two chromatic axes. The orientation of the chromatic axes is specified using a free rotation $\theta$ about the luminosity axis. The only restriction on the free parameter $\theta$ is that which results from the requirement for the matrix to consist entirely of integer values within a specific range. The question remains: is it possible to achieve a similar result without restricting one of the axes?

In this work, we assumed that an 8-bit per-channel value of $(255, 255, 255)$ corresponds with white. This, however is a result of the pre-processing on the iPhone. Extending this work to other devices or accessing the iPhone's RAW camera feed, it may be desirable to use a different point for the camera's white point. Another reason is to adjust for local ambient light conditions; it is common in digital photography to choose a pixel value from a reference white object to set a white point for the image, allowing a color correction to be performed.

An arbitrary rotation is determined by three angles, and so it's conceivable that the work of Appendix A could be repeated for this arbitrary rotation. Whilst it may be possible, an alternative approach may be preferable given the complexity of the general rotation equations. First we assume that the solution exists, then form an initial guess at the solution by quantizing the floating point representation of the matrix. Next, we find the maximum error produced by our approximate quantized rotation by rotating the corners of the RGB cube and comparing the results, then numerically search around this approximation for improvements. The question that remains is what search area should be included.

Because the general rotation matrix can be split into the product of three individual matrices, each of which is dependent on only one of the three free parameters, the work in Appendix A could be repeated relatively straightforwardly for each of these three individual matrices, and from this --- for a given quantization criteria --- a minimum and a maximum angle step size (Figure \ref{fig:PtbToChan}). If we have for the separate matrices a minimum and a maximum step size, it's a reasonable idea to search around our initial approximation in steps of the minimum step size to either side out to a maximum distance. For each of the three angles, we search within a cube with sides defined by the maximum step sizes for each of those rotation matrices on a cubic grid defined by their minimum step sizes. The result of this is that there is a relatively small set of possible optimized choices. Although this is not solving the problem to the same degree that was done in Appendix A, where we reduced it to a mathematical function, it provides a practical way of achieving the same results without excessive computational effort in the set-up. 

%\subsection{Multi-Channel Color Spaces}\label{sec:MultiChannelColorSpaces}
%
%As mentioned in Chapter 1, the RGB color-space is an approximation to the full spectrum of light. However, cameras which are not confined to the three-channel approximation are becoming commercially available (Figure \ref{fig:MultiSpectralCameras}); such a multi-channel image contains far more information about the objects in frame. So where the camera is being used for computer vision tasks the chromatic space can be used to discriminate between different objects and surfaces in ways which are undetectable in the standard RGB color-space.
%
%\begin{figure}
%  \centering
%  \begin{tabular}{cc}
%  \subfloat[Triwave EC701.]{
%    \includegraphics[width=.49\textwidth]{Chapter5/Figs/multispec-triwave.jpg}
%    }&
%  \subfloat[HyperCam by University of Washington and Microsoft Research.]{
%    \includegraphics[width=.49\textwidth]{Chapter5/Figs/multispec-hypercam.jpg}
%    } \\
%    \multicolumn{2}{c}{
%    \subfloat[Optec multi-spectral camera.]{
%        \includegraphics[width=.49\textwidth]{Chapter5/Figs/multispec-optec.jpg}
%    }
%    }
%  \end{tabular}
%  \caption{A variety of multi-spectral cameras currently in use.}
%  \label{fig:MultiSpectralCameras}
%\end{figure}
%
%The basic design of these multi-spectral cameras is essentially the same as for the RGB cameras where the spectrum of light on each point in the scene (roughly speaking; see Chapter 1) is represented by a combination of Gaussians, the only difference being we have more Gaussians. Each channel can therefore be considered an orthogonal axis in a multi-dimensional color-space. The multi-dimensional color-space contains a point which corresponds to white, so it is just as possible to construct a color-space with a luminosity axis and $n-1$ chromatic axes for an $n$-channel image. Transformation to this color-space is defined by a multi-dimensional rotation; this rotation could be optimized in a similar way to that presented in Chapter 2. However, our requirement in Chapter 2 was that the immediate result of the rotation would fit into a data type no more than twice the bit depth of each channel. An $n$-channel color-space will be rotated by an $n$ x $n$ rotation matrix; if we have $m$ bits per channel and the quantized rotation matrix is expressed in $l$-bit integers, the result is $l + m + (n-1) \leqslant 2m$. For the larger values of $n$, the rotation matrix would have to be expressed in smaller data types, so aside from special rotation angles which just happen to produce integer rotation matrices, it's unlikely that our criterion can be met. So for multi-channel images to still be able to use integer rotation matrices, the target data type will undoubtedly have to be significantly larger. 

\subsection{Using the RAW Image From the Camera}\label{sec:UsingTheRAWImageFromTheCamera}
The RAW camera image contains all the information captured by the camera, but without any pre-processing. Individual CCDs have different sensitivities in each of their channels, and so the corresponding bit depth of each channel may actually vary. This is likely to be the case for the iPhone's camera as we found in Chapter 3 that certain RGB values appear to be inaccessible. These inaccessible values also appear to be relatively evenly-spread throughout the RGB cube, which produced what we referred to as "speckling" in the statistics gathered in Chapter 3 as seen in Figure \ref{fig:Despeckle_the_Bins}. 
A possible explanation for the "speckling" is that one or more of the iPhone camera's channels are not truly captured at an 8-bit depth, but a smaller range of values which the CCD actually captures is stretched to fit the full 8-bit range. It is also entirely possible that one or more of the channels may actually be captured at a higher bit depth than 8-bit. But in terms of the algorithm, we now need to allow each channel in the source to have a variable bit depth. This will affect several things; it changes the integer range of allowed values in a rotation matrix and the algorithm for determining the optimal angle could be adapted by weighting the channels appropriately. But otherwise, adapting to a RAW camera image should be relatively straightforward.

\section{Implementation Improvements}\label{sec:ImplementationImprovements}

\subsection{An Empirical WoBo Algorithm}\label{sec:EmpicialWoBoAlgorithm}
The White-out Black-out algorithm outlined in Section \ref{sec:WhiteoutAndBlackout} uses theoretical values based on the extent of the data type used to store the RGB image. Looking at the empirical results presented in Figure \ref{fig:WhitePoint}, there's more to the WoBo behaviour than simply saturating the data type; the performance of the CCD changes with overall ambient light, and the pre-processing performed on the device also has an effect which takes the pixel values away from the native data point values using the data type extent. 

Although it may be possible to reverse-engineer both the pre-processing and CCD characteristics simultaneously, this would be a difficult task at best. However, if we were able to obtain the RAW image from the CCD, then repeatedly performing empirical tests, such as those performed to generate Figure \ref{fig:WhitePoint}, the CCD characteristics could be obtained. If we know how the CCD detects objects of a certain color under different ambient luminosities, then a more nuanced algorithm can be used to determine if a certain pixel value has suffered from white-out or black-out.

\subsection{Auto-Adjusting the Variance}\label{sec:AutoAdjustingTheVariance}
The algorithm presented in Chapter 4 uses a reference image to float the mean, yet the algorithm makes no attempt to adjust the variance to local ambient light conditions. This is found to work well, however it was initially found that using a standard deviation 2.3 times larger than the value found in Chapter 3 produced better results (see Figure \ref{fig:RelaxedSigma}). Although the algorithm was tested against a selection of background colors, backgrounds which are notably close to skin colors were avoided. For practical applications, it may therefore be useful to use a value for the standard deviation close to that used in Chapter 3.

The images chosen to be included in this document were taken against a green background because that gave the cleanest results; a value of 2.3$\sigma$ produced good results in this ideal background condition, so we'll choose 2.3 as the upper limit and 1$\sigma$ as the lower limit. We have a reference image where we have a good idea that there is a finger, and a portion of that finger is within a known frame, so the lower criteria is that nearly all, if not all, the pixel values within that frame are categorized as skin using the new value for the standard deviation, so for a choice between 1$\sigma$ and 2.3$\sigma$, we can "score" the choice as follows:

\begin{enumerate}
\item The proportion of pixels within the known skin region successfully classified as skin.
\item The number of edge points found using the Filament fill algorithm which are classified as bad edge points which also lie outside the approximated digit edge.
\item The number of edge points found by Filament fill which are classified as bad edge points, but which lie within the edges of the digit.
\end{enumerate}

So, a score where measure 1 is too low suggests that $\sigma$ should be increased, as pixel values which we know to be skin are classified as not skin; if measure 2 is too high, then we need to reduce the value of $\sigma$; and if measure 3 is too high, then we need to increase the value of $\sigma$.

This suggests a way that an algorithm could be developed which appropriately adjusts the value of $\sigma$ striking a compromise between false negatives and false positives. The actual values, thresholds and criteria used can best be determined empirically.

One way of proceeding may be to combine the three measures by taking the weighted product of measures 2 and 3 with the weighted reciprocal of measure 1. We combine the measures in this fashion and minimize the result with respect to $\sigma$.

\subsection{Statistics Gathering Method for the App}\label{sec:StatisticsGatheringMethod}
The statistical model presented in Chapter 3 was found using algorithms written in MATLAB. There is no real barrier to the entire method being written in C++ using the OpenCV library running on an iPhone. The only user input needed would be to select which color is the background and which is the skin.

%
%\subsection{Improved ICWaS Alignment}\label{sec:ImprovedICWaSAlignment}
%To align successive frames, the ICWaS developed in Chapter 4 uses OpenCV's template aligning routines which operates by performing a translation of one frame and computing a per-pixel difference. This works because the alignment is already quite good. There are two ways in which this can be improved; the allowed transforms can be increased to include rotation and perspective adjustments, and the alignment methodology can be improved to utilize more constant features of the fingertip image than the per-pixel approach. 
%
%It's a simple matter to allow for a more general transform, however the per-pixel based comparison is wholly inappropriate to find a more general transform as we would have to try all possible results of the transform and compare them. This would be computationally far too expensive. So, to find a more general transform, we must turn each frame into a set of feature points which can be put into a correspondence, or perform some image transform which simplifies the alignment problem.
%
%
%\subsection{More Designed Metric for Mechanical Stress}\label{sec:MoreDesignedMetricForMechanicalStress}
%The metric method which is very briefly presented at the end of Chapter 4 (see Figures \ref{fig:ICWaSResultJSkin}, \ref{fig:ICWaSResultFSkin} and \ref{fig:ICWaSResultNSkin}) relies on a pixel-to-pixel comparison; it is well known that pixel-based methods are unreliable for detecting movement, and our mechanical stress measurement relies on detecting the flow of blood. The metric should therefore account for neighboring pixels. 
%
%There are many possible methodologies for detecting movement which would improve the pixel-based method of Chapter 4. Mechanical stress tends to move pooled blood from one area to another within the stressed tissue. The upshot of this is that as one portion of the image loses blood, another gains it. This is different to the circulatory blood, which is pumped in through the arteries and capillaries and flows out through the veins. A better metric may be to compare blobs in the negative and positive chromatic difference images. To be clear, the algorithm would find blobs using a negative thresholding and a positive thresholding; the largest blobs in each of these spaces would be considered good indicators of pooled blood flow.
%
%Another possibility arises from incidental observations of the pooled blood flow images; for each individual's digit, the pooled blood flow pattern is fairly consistent under similar mechanical stress conditions. This suggests that it may be possible to construct a blood flow model for a given digit. Regarding the app, we can imagine generalizing the reference image idea to asking the user to press on a surface several times, allowing the blood flow model to be generated for that digit.

\section{Conclusion}\label{sec:ConclusionCh5}

The finger press algorithm is a simple application to demonstrate the viability of detecting blood movement using a standard camera on a mobile device. Many previous authors have dismissed using bespoke color-spaces for such applications simply because the color-space transform itself is computationally intensive and significantly loses information when applied using standard library color-spaces.

It is hoped that, with the rigorous optimization of the transform itself  that these color-spaces can see greater application in the future, although it's recognized that the level of attention that's currently required to make the routine as efficient at the one designed herein will likely be beyond the patience of many practitioners, some of whom are medical professionals and not computer scientists. Also, it is hoped that the work presented here has addressed the concerns about the associated loss of information.

Finally, the use of a 2D Gaussian model for the skin color-space has been the source of much debate; some authors insist that the statistical model be extended to all three dimensions (\cite{Shin2002a}), but hopefully it can be seen from this work that the luminosity channel can be effectively removed from consideration because its influence on the statistics is an artefact due to white-out and black-out. In fairness to the authors who have argued for its inclusion in the model, the luminosity is considered by the Quaternary Pixel Classification outlined in Section~\ref{sec:QuaternaryPixelClassification}, however this part of the algorithm is theoretically derived from the two-dimensional chromatic statistics. To be clear, the authors who argued for the full three-dimensional model were correct in that luminosity has an effect, however they were including in the skin model camera effects which can be compensated for by application of theory. 

The secondary controversy is the computational cost of applying a 2D Gaussian, which has led to authors using elliptical functions, rectangular functions, etc. The algorithm developed in Chapter 2 takes advantage of all these methods to produce a composite algorithm which very efficiently applies a 2D Gaussian distribution. The central trick employed was to use a bespoke color-space in which the long axis of the 2D Gaussian is aligned parallel to one of the axes of the color-space. This means that the 2D Gaussian is functionally expressed as the product of one-dimensional Gaussians. The advantages of using a three-dimensional model would appear to be re-gained using the Quaternary Pixel Classification algorithm, and by judicious collection of the skin statistics as seen in Chapter 3. 

In conclusion, we have presented a rigorous statistical method and model for human skin detection which has, to my understanding, addressed all the concerns expressed in the current literature.